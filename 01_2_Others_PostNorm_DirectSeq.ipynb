{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c039ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from Util_def import *\n",
    "from Util_model import *\n",
    "from pypfopt import (\n",
    "    EfficientFrontier,\n",
    "    risk_models,\n",
    "    expected_returns,\n",
    "    objective_functions,\n",
    ")\n",
    "import pickle as pkl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd6b874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2515 entries, 2015-01-02 to 2024-12-30\n",
      "Data columns (total 33 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   SHV     2515 non-null   float64\n",
      " 1   BND     2515 non-null   float64\n",
      " 2   BNDX    2515 non-null   float64\n",
      " 3   JNK     2515 non-null   float64\n",
      " 4   VT      2515 non-null   float64\n",
      " 5   VEA     2515 non-null   float64\n",
      " 6   IEMG    2515 non-null   float64\n",
      " 7   VOO     2515 non-null   float64\n",
      " 8   QQQ     2515 non-null   float64\n",
      " 9   DIA     2515 non-null   float64\n",
      " 10  VGK     2515 non-null   float64\n",
      " 11  EWJ     2515 non-null   float64\n",
      " 12  MCHI    2515 non-null   float64\n",
      " 13  THD     2515 non-null   float64\n",
      " 14  VNM     2515 non-null   float64\n",
      " 15  INDA    2515 non-null   float64\n",
      " 16  RXI     2515 non-null   float64\n",
      " 17  KXI     2515 non-null   float64\n",
      " 18  IXC     2515 non-null   float64\n",
      " 19  IXG     2515 non-null   float64\n",
      " 20  IXJ     2515 non-null   float64\n",
      " 21  EXI     2515 non-null   float64\n",
      " 22  IXN     2515 non-null   float64\n",
      " 23  IXP     2515 non-null   float64\n",
      " 24  JXI     2515 non-null   float64\n",
      " 25  ITA     2515 non-null   float64\n",
      " 26  ICLN    2515 non-null   float64\n",
      " 27  SKYY    2515 non-null   float64\n",
      " 28  SMH     2515 non-null   float64\n",
      " 29  REET    2515 non-null   float64\n",
      " 30  IGF     2515 non-null   float64\n",
      " 31  PDBC    2515 non-null   float64\n",
      " 32  GLD     2515 non-null   float64\n",
      "dtypes: float64(33)\n",
      "memory usage: 668.0 KB\n",
      "None\n",
      "==================================================\n",
      "Min Date: 2015-01-02 00:00:00\n",
      "Max Date: 2024-12-30 00:00:00\n",
      "Start Rebalance Year: 2020\n",
      "Average number of trading days per month: 21 days\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "ETF_list = [\n",
    "    'SHV',\n",
    "    'BND', 'BNDX', 'JNK',\n",
    "    'VT', 'VEA', 'IEMG',\n",
    "    'VOO', 'QQQ', 'DIA', 'VGK', 'EWJ', 'MCHI', 'THD', 'VNM', 'INDA',\n",
    "    'RXI', 'KXI', 'IXC', 'IXG', 'IXJ', 'EXI', 'IXN', 'IXP', 'JXI',\n",
    "    'ITA', 'ICLN', 'SKYY', 'SMH',\n",
    "    'REET', 'IGF', 'PDBC', 'GLD'\n",
    "]\n",
    "\n",
    "# 5 years data\n",
    "startDate = dt.datetime(2015, 1, 1)\n",
    "endDate = dt.datetime(2024, 12, 31)\n",
    "\n",
    "start_rebalance_year = 2020  # startDate.year + 3\n",
    "\n",
    "# data = getData(ETF_list, startDate, endDate)\n",
    "# data.fillna(method='ffill', inplace=True)\n",
    "# data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# read csv\n",
    "data = pd.read_csv('33_ETF_data.csv', index_col='Date', parse_dates=True)\n",
    "print(data.info())\n",
    "avg_days = avg_days_per_month(data)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Min Date:\", data.index.min())\n",
    "print(\"Max Date:\", data.index.max())\n",
    "print(\"Start Rebalance Year:\", start_rebalance_year)\n",
    "print(f\"Average number of trading days per month: {avg_days}\", \"days\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a827cd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 33)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Portfolio Type ######\n",
    "long_only = tuple([0,1])\n",
    "long_short = tuple([-1,1])\n",
    "\n",
    "port_type = long_only \n",
    "\n",
    "###### Adding Constraints ######\n",
    "# Asset Mapping\n",
    "asset_map = {\n",
    "    'SHV': 'Cash_Equivalent',\n",
    "    \n",
    "    'BND': 'Fixed_Income',\n",
    "    'BNDX': 'Fixed_Income',\n",
    "    'JNK': 'Fixed_Income',\n",
    "\n",
    "    'VT': 'Equity',\n",
    "    'VEA': 'Equity',\n",
    "    'IEMG': 'Equity',\n",
    "\n",
    "    'VOO': 'Equity',\n",
    "    'QQQ': 'Equity',\n",
    "    'DIA': 'Equity',\n",
    "    'VGK': 'Equity',\n",
    "    'EWJ': 'Equity',\n",
    "    'MCHI': 'Equity',\n",
    "    'THD': 'Equity',\n",
    "    'VNM': 'Equity',\n",
    "    'INDA': 'Equity',\n",
    "\n",
    "    'RXI': 'Equity',\n",
    "    'KXI': 'Equity',\n",
    "    'IXC': 'Equity',\n",
    "    'IXG': 'Equity',\n",
    "    'IXJ': 'Equity',\n",
    "    'EXI': 'Equity',\n",
    "    'IXN': 'Equity',\n",
    "    'IXP': 'Equity',\n",
    "    'JXI': 'Equity',\n",
    "\n",
    "    'ITA': 'Equity',\n",
    "    'ICLN': 'Equity',\n",
    "    'SKYY': 'Equity',\n",
    "    'SMH': 'Equity',\n",
    "\n",
    "    'REET': 'Alternatives',\n",
    "    'IGF': 'Alternatives',\n",
    "    'PDBC': 'Alternatives',\n",
    "    'GLD': 'Alternatives',\n",
    "}\n",
    "\n",
    "### Aggressive Portfolio ###\n",
    "asset_lower_aggressive = {\n",
    "    'Cash_Equivalent': 0.0,\n",
    "    'Fixed_Income': 0.0,\n",
    "    'Equity': 0.55,\n",
    "    'Alternatives': 0.0}\n",
    "asset_upper_aggressive = {\n",
    "    'Cash_Equivalent': 0.4,\n",
    "    'Fixed_Income': 0.3,\n",
    "    'Equity': 0.9,\n",
    "    'Alternatives': 0.3}\n",
    "\n",
    "len(ETF_list), len(asset_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11536836",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303a1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the results\n",
    "output_dir = 'Results'\n",
    "model_type = 'Transformer'  # 'Transformer', 'LSTM', 'BiLSTM', 'GRU', 'BiGRU', '2DCNN', '1DCNN'\n",
    "pe_type = 'OriPE'          # 'OriPE', 'Time2Vec', 'ConvSPE', 'SineSPE', 'TemporalPE', 'LearnablePE', 'AbsolutePE', 'tAPE'\n",
    "pre_post = 'PostNorm'       # 'PostNorm' or 'PreNorm'\n",
    "n_temp = 1.0\n",
    "train_type = f'01_2_{model_type}_{pe_type}_{pre_post}_temp_{n_temp}'  # '01_1', '01_2', '01_3', etc.\n",
    "run_no = 10\n",
    "\n",
    "\n",
    "# --- เริ่มโค้ดสำหรับบันทึก Excel ---\n",
    "results_excel_path = f\"{output_dir}/Others/{model_type}/{train_type}/01_Results_{model_type}_{pe_type}_{pre_post}_run_{run_no}.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe7b3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, LayerNormalization, MultiHeadAttention, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from dateutil.parser import parse\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "from tensorflow.keras.models import Model as KModel, Sequential\n",
    "from tensorflow.keras import layers, Model as KModel\n",
    "import cvxpy as cp\n",
    "import cvxopt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  details = tf.config.experimental.get_device_details(gpus[0])\n",
    "  print(\"GPU details: \", details)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2a2dd",
   "metadata": {},
   "source": [
    "Quadratic Programming (QP) method - Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, max_weight=1, asset_map=None, asset_lower=None, asset_upper=None, port_type=None):\n",
    "        # self.data = None\n",
    "        self.model = None\n",
    "        self.max_weight = max_weight\n",
    "        self.asset_map = asset_map or {}\n",
    "        self.asset_lower = asset_lower or {}\n",
    "        self.asset_upper = asset_upper or {}\n",
    "        self.port_type = port_type\n",
    "        self.asset_columns = None\n",
    "\n",
    "    def _create_constraint_matrices(self, columns):\n",
    "        \"\"\"Create constraint matrices for asset‐type bounds.\"\"\"\n",
    "        self.asset_columns = columns\n",
    "        \n",
    "        asset_types = {}\n",
    "        for asset in columns:\n",
    "            t = self.asset_map.get(asset, \"Unknown\")\n",
    "            asset_types.setdefault(t, []).append(asset)\n",
    "        \n",
    "        mats = []\n",
    "        lbs = []\n",
    "        ubs = []\n",
    "        names = []\n",
    "        for t, assets in asset_types.items():\n",
    "            vec = np.zeros(len(columns), dtype=float)\n",
    "            for a in assets:\n",
    "                idx = columns.get_loc(a)\n",
    "                vec[idx] = 1.0\n",
    "            mats.append(vec)\n",
    "            lbs.append(self.asset_lower.get(t, 0.0))\n",
    "            ubs.append(self.asset_upper.get(t, 1.0))\n",
    "            names.append(t)\n",
    "        \n",
    "        self.constraint_matrix = np.vstack(mats)        # shape = (n_types, n_assets)\n",
    "        self.lower_bounds = np.array(lbs, dtype=float)  # shape = (n_types,)\n",
    "        self.upper_bounds = np.array(ubs, dtype=float)  # shape = (n_types,)\n",
    "        self.asset_type_names = names\n",
    "        \n",
    "    # Final New QP with two phases - this allows for hard constraints on zero weights\n",
    "    def _apply_constraints_final(self, weights):\n",
    "        \"\"\"\n",
    "        ใช้ Quadratic Programming เพื่อบังคับ:\n",
    "        • Σw = 1\n",
    "        • per-asset cap 0.30 (SHV 0.40)\n",
    "        • asset_lower / asset_upper\n",
    "        • ลดการขยับจาก w0 โดยเฉพาะตำแหน่งที่ w0 == 0\n",
    "        \"\"\"\n",
    "        w0 = np.asarray(weights, float).copy()\n",
    "        n  = w0.size\n",
    "\n",
    "        # ---------- สร้างขอบรายตัว --------------------------------------------\n",
    "        ub = np.full(n, 0.30)\n",
    "        if self.asset_columns is not None and \"SHV\" in self.asset_columns:\n",
    "            ub[self.asset_columns.get_loc(\"SHV\")] = 0.40\n",
    "\n",
    "        lb = np.full(n, self.port_type[0])      # long_only → 0, long/short → -1\n",
    "        # (ถ้ามีพอร์ตชนิดอื่นปรับได้ตาม self.port_type)\n",
    "\n",
    "        # ---------- ตัวแปร QP ---------------------------------------------------\n",
    "        w = cp.Variable(n)\n",
    "\n",
    "        constraints = [\n",
    "            cp.sum(w) == 1,\n",
    "            w >= lb,\n",
    "            w <= ub\n",
    "        ]\n",
    "\n",
    "        # ---------- ข้อจำกัดรายหมวด -------------------------------------------\n",
    "        if hasattr(self, \"constraint_matrix\"):\n",
    "            C = self.constraint_matrix           # shape (n_types, n_assets)\n",
    "            constraints += [\n",
    "                C @ w >= self.lower_bounds,\n",
    "                C @ w <= self.upper_bounds\n",
    "            ]\n",
    "\n",
    "        # ---------- Objective: min Σ α_i (w_i - w0_i)^2 ------------------------\n",
    "        eps = 1e-4\n",
    "        alpha = 1.0 / (w0 + eps)        # ช่องที่ w0=0 จะถูกลงโทษมาก\n",
    "        obj   = cp.Minimize(cp.sum(cp.multiply(alpha, cp.square(w - w0))))\n",
    "\n",
    "        prob = cp.Problem(obj, constraints)\n",
    "\n",
    "        # เลือก solver ที่รองรับ QP\n",
    "        try:\n",
    "            prob.solve(solver=cp.OSQP)  # หรือ ECOS_BB / SCS\n",
    "        except cp.error.SolverError:\n",
    "            prob.solve(solver=cp.ECOS)\n",
    "\n",
    "        # ถ้าแก้ไม่ได้ (infeasible) กลับไปใช้วิธีเดิม\n",
    "        if w.value is None:\n",
    "            print(\"QP infeasible, falling back to greedy method.\")\n",
    "            return super()._apply_constraints(weights)\n",
    "\n",
    "        return np.asarray(w.value).flatten()\n",
    "    # ===== END V.2 =====\n",
    "    \n",
    "    # _apply_constraints_row_cvxpy\n",
    "    def _apply_constraints_row_cvxpy(self, weights_row, tol_zero=1e-6, lambda_zero=100.0):\n",
    "        \"\"\"\n",
    "        Applies two-phase QP constraints to a single row of weights.\n",
    "        - Phase 1: Attempts to solve with a hard lock on zero-weight assets.\n",
    "        - Phase 2: If Phase 1 is infeasible, it releases the lock and instead\n",
    "                   applies a heavy penalty for moving away from zero.\n",
    "        \"\"\"\n",
    "        # ---------- Data Preparation for a single row ----------\n",
    "        w0 = np.clip(weights_row, *self.port_type)\n",
    "        n = len(w0)\n",
    "        lower_w, _ = self.port_type\n",
    "\n",
    "        # Upper bound vector (0.30, except 0.40 for SHV)\n",
    "        ub_vec = np.full(n, 0.30)\n",
    "        if 'SHV' in self.asset_columns:\n",
    "            shv_idx = self.asset_columns.get_loc('SHV')\n",
    "            ub_vec[shv_idx] = 0.40\n",
    "\n",
    "        # Indices of assets with near-zero initial weights\n",
    "        zero_idx = np.where(w0 <= tol_zero)[0]\n",
    "\n",
    "        # ---------- Phase 1: Solve QP with locked zeros ----------\n",
    "        w_p1 = cp.Variable(n)\n",
    "        \n",
    "        # Base constraints + hard lock on zeros\n",
    "        cons_p1 = [w_p1 >= lower_w, w_p1 <= ub_vec]\n",
    "        if len(zero_idx) > 0:\n",
    "            cons_p1.append(w_p1[zero_idx] == 0)\n",
    "\n",
    "        # Add sum-to-one and group constraints\n",
    "        if self.port_type == long_only:\n",
    "            cons_p1.append(cp.sum(w_p1) == 1)\n",
    "        if hasattr(self, 'constraint_matrix'):\n",
    "            cm, lb, ub = self.constraint_matrix, self.lower_bounds, self.upper_bounds\n",
    "            cons_p1.extend([cm @ w_p1 >= lb, cm @ w_p1 <= ub])\n",
    "        \n",
    "        obj_p1 = cp.Minimize(cp.sum_squares(w_p1 - w0))\n",
    "        prob_p1 = cp.Problem(obj_p1, cons_p1)\n",
    "        prob_p1.solve(solver=cp.OSQP, warm_start=True)\n",
    "\n",
    "        if prob_p1.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "            return np.array(w_p1.value).flatten()\n",
    "\n",
    "        # ---------- Phase 2: Solve QP with penalty if Phase 1 failed ----------\n",
    "        print(\"⚠️ Strict (zero-locked) QP failed. Falling back to penalty-based QP.\")\n",
    "        w_p2 = cp.Variable(n)\n",
    "        \n",
    "        # Base constraints without the hard lock\n",
    "        cons_p2 = [w_p2 >= lower_w, w_p2 <= ub_vec]\n",
    "        if self.port_type == long_only:\n",
    "            cons_p2.append(cp.sum(w_p2) == 1)\n",
    "        if hasattr(self, 'constraint_matrix'):\n",
    "            cm, lb, ub = self.constraint_matrix, self.lower_bounds, self.upper_bounds\n",
    "            cons_p2.extend([cm @ w_p2 >= lb, cm @ w_p2 <= ub])\n",
    "            \n",
    "        # Softer objective with penalty for moving zero-weight assets\n",
    "        penalty = np.ones(n)\n",
    "        penalty[zero_idx] = lambda_zero\n",
    "        sqrt_p = np.sqrt(penalty)\n",
    "        obj_p2 = cp.Minimize(cp.sum_squares(cp.multiply(sqrt_p, w_p2 - w0)))\n",
    "        \n",
    "        prob_p2 = cp.Problem(obj_p2, cons_p2)\n",
    "        prob_p2.solve(solver=cp.OSQP, warm_start=True)\n",
    "\n",
    "        if prob_p2.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "            return np.array(w_p2.value).flatten()\n",
    "        else:\n",
    "            # Final fallback if both phases fail\n",
    "            print(f\"❌ Warning: Both QP attempts failed (Final status: {prob_p2.status}). Using fallback normalization.\")\n",
    "            s = w0.sum()\n",
    "            return (w0 / s) if s > 0 else np.ones_like(w0) / n\n",
    "    # ===== END =====\n",
    "\n",
    "    def build(self, input_shape, outputs, PE_type=pe_type):\n",
    "\n",
    "        if model_type == 'Transformer':\n",
    "            d_model = 512\n",
    "            num_heads = 8\n",
    "            ff_dim = 2048\n",
    "            num_layers = 6\n",
    "            dropout_rate = 0.3\n",
    "            spe_kernel_size = 5\n",
    "            seq_len, feature_dim = input_shape\n",
    "\n",
    "            inputs = layers.Input(shape=(seq_len, feature_dim))\n",
    "            # x = layers.Dense(d_model)(inputs)\n",
    "            # pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "            # x = x + pos_encoding\n",
    "            \n",
    "            # ====== Position Encoding ======\n",
    "            if PE_type == 'OriPE':\n",
    "                x = layers.Dense(d_model)(inputs)\n",
    "                pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "                x = x + pos_encoding[tf.newaxis, :]\n",
    "            elif PE_type == 'Time2Vec':\n",
    "                time_embedding = Time2Vector(seq_len)(inputs)\n",
    "                x = layers.Concatenate(axis=-1)([inputs, time_embedding])\n",
    "                x = layers.Dense(d_model)(x)\n",
    "            elif PE_type == 'ConvSPE':\n",
    "                x = layers.Dense(d_model)(inputs)\n",
    "                x = ConvSPE(d_model=d_model, kernel_size=spe_kernel_size)(x)\n",
    "            elif PE_type == 'SineSPE':\n",
    "                x = layers.Dense(d_model)(inputs)\n",
    "                x = SineSPE(d_model=d_model, max_len=seq_len + 100)(x)\n",
    "            elif PE_type == 'TemporalPE':\n",
    "                x = layers.Dense(d_model)(inputs)\n",
    "                x = TemporalPositionalEncoding(d_model=d_model, max_len=seq_len + 100)(x)\n",
    "            elif PE_type == 'LearnablePE':\n",
    "                x = layers.Dense(d_model)(inputs)\n",
    "                pos_encoding_layer = LearnablePositionalEncoding(d_model=d_model, max_len=seq_len + 100, dropout=dropout_rate)\n",
    "                x = pos_encoding_layer(x)\n",
    "            elif PE_type == 'AbsolutePE':\n",
    "                x = layers.Dense(d_model)(inputs)\n",
    "                pos_encoding_layer = AbsolutePositionalEncoding(d_model=d_model, max_len=seq_len + 100, dropout=dropout_rate)\n",
    "                x = pos_encoding_layer(x)\n",
    "            elif PE_type == 'tAPE':\n",
    "                x = layers.Dense(d_model)(inputs)\n",
    "                pos_encoding_layer = tAPE(d_model=d_model, max_len=seq_len + 100, dropout=dropout_rate)\n",
    "                x = pos_encoding_layer(x)\n",
    "            elif PE_type == 'TUPE':\n",
    "                x = layers.Dense(d_model)(inputs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown Positional Encoding type: {PE_type}\")\n",
    "\n",
    "            # ====== Transformer Encoder ======\n",
    "            if PE_type == 'TUPE':\n",
    "                attention_layer = TUPEMultiHeadAttention(d_model=d_model, num_heads=num_heads, max_len=seq_len + 100)\n",
    "            else:\n",
    "                attention_layer = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "\n",
    "            for _ in range(num_layers):\n",
    "                # ใช้ attention_layer ที่เรากำหนดไว้\n",
    "                attn_output = attention_layer(x, x, x) # Q, K, V\n",
    "                attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "                out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "                ffn = layers.Dense(ff_dim, activation=\"relu\")(out1)\n",
    "                ffn = layers.Dense(d_model)(ffn)\n",
    "                ffn_output = layers.Dropout(dropout_rate)(ffn)\n",
    "                x = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "            pooled = layers.GlobalAveragePooling1D()(x)      \n",
    "\n",
    "        elif model_type == 'LSTM':\n",
    "            # LSTM model parameters\n",
    "            lstm_units_1 = 516\n",
    "            lstm_units_2 = 256\n",
    "            lstm_units_3 = 128\n",
    "            lstm_units_4 = 64\n",
    "            lstm_units_5 = 32\n",
    "            dropout_rate = 0.3\n",
    "            l2_reg_kernel = 1e-4  # ค่าสำหรับ kernel_regularizer\n",
    "            l2_reg_recurrent = 1e-5 # ค่าสำหรับ recurrent_regularizer (อาจเริ่มด้วยค่าน้อยกว่า)\n",
    "\n",
    "            seq_len, feature_dim = input_shape\n",
    "            inputs = layers.Input(shape=(seq_len, feature_dim))\n",
    "            x = layers.LSTM(lstm_units_1, return_sequences=True, kernel_regularizer=regularizers.l2(l2_reg_kernel), recurrent_regularizer=regularizers.l2(l2_reg_recurrent))(inputs)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)  # Normalization after LSTM layer\n",
    "            x = layers.LSTM(lstm_units_2, return_sequences=True, kernel_regularizer=regularizers.l2(l2_reg_kernel), recurrent_regularizer=regularizers.l2(l2_reg_recurrent))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)  # Normalization after LSTM layer\n",
    "            x = layers.LSTM(lstm_units_3, return_sequences=True, kernel_regularizer=regularizers.l2(l2_reg_kernel), recurrent_regularizer=regularizers.l2(l2_reg_recurrent))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)  # Normalization after LSTM layer\n",
    "            x = layers.LSTM(lstm_units_4, return_sequences=True, kernel_regularizer=regularizers.l2(l2_reg_kernel), recurrent_regularizer=regularizers.l2(l2_reg_recurrent))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)  # Normalization after LSTM layer\n",
    "            x = layers.LSTM(lstm_units_5, return_sequences=False, kernel_regularizer=regularizers.l2(l2_reg_kernel), recurrent_regularizer=regularizers.l2(l2_reg_recurrent))(x)\n",
    "            pooled = layers.Dropout(dropout_rate)(x)\n",
    "            pooled = layers.LayerNormalization(epsilon=1e-6)(pooled)\n",
    "\n",
    "        elif model_type == 'BiLSTM':\n",
    "            # BiLSTM model parameters\n",
    "            lstm_units_1 = 516\n",
    "            lstm_units_2 = 256\n",
    "            lstm_units_3 = 128\n",
    "            lstm_units_4 = 64\n",
    "            lstm_units_5 = 32\n",
    "            dropout_rate = 0.3\n",
    "            l2_reg_kernel = 1e-4\n",
    "\n",
    "            seq_len, feature_dim = input_shape\n",
    "            inputs = layers.Input(shape=(seq_len, feature_dim))\n",
    "            x = layers.Bidirectional(layers.LSTM(lstm_units_1, return_sequences=True, kernel_regularizer=regularizers.l2(l2_reg_kernel)))(inputs)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)  # Normalization after LSTM layer\n",
    "            x = layers.Bidirectional(layers.LSTM(lstm_units_2, return_sequences=True, kernel_regularizer=regularizers.l2(l2_reg_kernel)))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)  # Normalization after LSTM layer\n",
    "            x = layers.Bidirectional(layers.LSTM(lstm_units_3, return_sequences =True, kernel_regularizer=regularizers.l2(l2_reg_kernel)))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)  # Normalization after LSTM layer\n",
    "            x = layers.Bidirectional(layers.LSTM(lstm_units_4, return_sequences=True, kernel_regularizer=regularizers.l2(l2_reg_kernel)))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)  # Normalization after LSTM layer\n",
    "            x = layers.Bidirectional(layers.LSTM(lstm_units_5, return_sequences=False, kernel_regularizer=regularizers.l2(l2_reg_kernel)))(x)\n",
    "            pooled = layers.Dropout(dropout_rate)(x)\n",
    "            pooled = layers.LayerNormalization(epsilon=1e-6)(pooled)\n",
    "        \n",
    "        elif model_type == 'RNN':\n",
    "            rnn_units = [512, 256, 128, 64, 32]\n",
    "            dropout_rate = 0.3\n",
    "            seq_len, feature_dim = input_shape\n",
    "            inputs = layers.Input(shape=(seq_len, feature_dim))\n",
    "            x = layers.SimpleRNN(rnn_units[0], return_sequences=True)(inputs)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)  # Normal\n",
    "            for i in range(1, len(rnn_units)):\n",
    "                x = layers.SimpleRNN(rnn_units[i], return_sequences=True)(x)\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "                x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "            pooled = layers.GlobalAveragePooling1D()(x)  # (batch_size, d_model)\n",
    "            pooled = layers.Dropout(dropout_rate)(pooled)\n",
    "            pooled = layers.LayerNormalization(epsilon=1e-6)(pooled)\n",
    "\n",
    "        elif model_type == '2DCNN':\n",
    "            # 2D-CNN model parameters\n",
    "            filters = [32, 64, 128, 256, 512]\n",
    "            kernel_size = (3, 3)\n",
    "            dropout_rate = 0.3\n",
    "\n",
    "            seq_len, feature_dim = input_shape\n",
    "            inputs = layers.Input(shape=(seq_len, feature_dim))\n",
    "            x = layers.Reshape((seq_len, feature_dim, 1))(inputs)\n",
    "\n",
    "            for f in filters:\n",
    "                # ลำดับที่แนะนำ: Conv -> Batch Norm -> Activation -> Pooling -> Dropout\n",
    "                x = layers.Conv2D(filters=f,\n",
    "                                kernel_size=kernel_size,\n",
    "                                padding='same',\n",
    "                                use_bias=False)(x) # ปิดการใช้ bias เมื่อมี Batch Norm\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Activation('relu')(x) # ใช้ 'relu'\n",
    "                x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "            pooled = layers.Flatten()(x)\n",
    "\n",
    "        elif model_type == '1DCNN':\n",
    "            # 1D-CNN model parameters\n",
    "            filters = [32, 64, 128, 256, 512]\n",
    "            kernel_size = 3\n",
    "            dropout_rate = 0.3\n",
    "\n",
    "            seq_len, feature_dim = input_shape\n",
    "            inputs = layers.Input(shape=(seq_len, feature_dim))\n",
    "            x = inputs  \n",
    "            for f in filters:\n",
    "                # ลำดับที่แนะนำ: Conv -> Batch Norm -> Activation -> Pooling -> Dropout\n",
    "                x = layers.Conv1D(filters=f,\n",
    "                                kernel_size=kernel_size,\n",
    "                                padding='same',\n",
    "                                # dilation_rate=... # ลองเพิ่ม dilation_rate ที่นี่ได้ เช่น 2 หรือ 4\n",
    "                                use_bias=False)(x) # ปิด bias เมื่อใช้ Batch Norm\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Activation('relu')(x)\n",
    "                x = layers.MaxPooling1D(pool_size=2)(x) # ลดขนาด Sequence ลงครึ่งหนึ่ง\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "            pooled = layers.GlobalAveragePooling1D()(x)       \n",
    "            \n",
    "        elif model_type == 'GRU':\n",
    "            # GRU model parameters\n",
    "            gru_units_1, gru_units_2, gru_units_3, gru_units_4, gru_units_5= 512, 256, 128, 64, 32\n",
    "            dropout_rate = 0.3\n",
    "\n",
    "            seq_len, feature_dim = input_shape\n",
    "            inputs = layers.Input(shape=(seq_len, feature_dim))\n",
    "            x = layers.GRU(gru_units_1, return_sequences=True)(inputs)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "            x = layers.GRU(gru_units_2, return_sequences=True)(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "            x = layers.GRU(gru_units_3, return_sequences=True)(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "            x = layers.GRU(gru_units_4, return_sequences=True)(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "            x = layers.GRU(gru_units_5, return_sequences=False)(x)\n",
    "            pooled = layers.Dropout(dropout_rate)(x)\n",
    "            pooled = layers.LayerNormalization(epsilon=1e-6)(pooled)\n",
    "\n",
    "        elif model_type == 'BiGRU':\n",
    "            # BiGRU model parameters\n",
    "            gru_units_1, gru_units_2, gru_units_3, gru_units_4, gru_units_5 = 512, 256, 128, 64, 32\n",
    "            dropout_rate = 0.3\n",
    "\n",
    "            seq_len, feature_dim = input_shape\n",
    "            inputs = layers.Input(shape=(seq_len, feature_dim))\n",
    "            x = layers.Bidirectional(layers.GRU(gru_units_1, return_sequences=True))(inputs)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "            x = layers.Bidirectional(layers.GRU(gru_units_2, return_sequences=True))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "            x = layers.Bidirectional(layers.GRU(gru_units_3, return_sequences=True))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "            x = layers.Bidirectional(layers.GRU(gru_units_4, return_sequences=True))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "            x = layers.Bidirectional(layers.GRU(gru_units_5, return_sequences=False))(x)\n",
    "            pooled = layers.Dropout(dropout_rate)(x)\n",
    "            pooled = layers.LayerNormalization(epsilon=1e-6)(pooled)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "\n",
    "        #  ===== Final output layer =====\n",
    "        outputs_layer = layers.Dense(outputs)(pooled)\n",
    "        outputs_layer = TemperatureSoftmax(temperature=n_temp)(outputs_layer) \n",
    "          \n",
    "        # outputs_layer = layers.Dense(outputs, activation=TemperatureSoftmax(3.0))(pooled)  \n",
    "\n",
    "        model = KModel(inputs=inputs, outputs=outputs_layer)\n",
    "                     \n",
    "\n",
    "        \n",
    "        def sharpe_loss(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            คำนวณ Sharpe Ratio Loss สำหรับ Batch ของข้อมูล\n",
    "            \n",
    "            Args:\n",
    "                y_true (tf.Tensor): ผลตอบแทนรายวันในอนาคต (future daily returns)\n",
    "                                    Shape: (batch_size, forecast_horizon, n_assets)\n",
    "                y_pred (tf.Tensor): น้ำหนักพอร์ตที่โมเดลทำนาย (predicted weights)\n",
    "                                    Shape: (batch_size, n_assets)\n",
    "            \"\"\"\n",
    "            ## y_pred (weights) มี shape (batch_size, n_assets)\n",
    "            ## y_true (returns) มี shape (batch_size, forecast_horizon, n_assets)\n",
    "            #print(f\"\\n===== Calculating Sharpe Ratio Loss =====\")\n",
    "            #print(\"y_true (returns) shape:\", y_true.shape)\n",
    "            #print(\"y_pred (weights) shape:\", y_pred.shape)\n",
    "\n",
    "            # ขยายมิติของ y_pred เพื่อให้คูณกับ y_true ได้\n",
    "            # (batch_size, n_assets) -> (batch_size, 1, n_assets)\n",
    "            weights_expanded = tf.expand_dims(y_pred, axis=1)\n",
    "            #print(\"weights_expanded shape:\", weights_expanded.shape)\n",
    "\n",
    "            # คำนวณผลตอบแทนรายวันของพอร์ตสำหรับแต่ละ sample ใน batch\n",
    "            # Shape: (batch_size, forecast_horizon, n_assets) * (batch_size, 1, n_assets)\n",
    "            # -> (batch_size, forecast_horizon, n_assets)\n",
    "            # -> reduce_sum(axis=2) -> (batch_size, forecast_horizon)\n",
    "            portfolio_daily_returns = tf.reduce_sum(tf.multiply(y_true, weights_expanded), axis=2)\n",
    "            #print(\"portfolio_daily_returns shape:\", portfolio_daily_returns.shape)\n",
    "\n",
    "            # คำนวณ Mean และ Std ของผลตอบแทนรายวันสำหรับแต่ละ sample ใน batch (ตามแกน forecast_horizon)\n",
    "            mean_daily_return = tf.reduce_mean(portfolio_daily_returns, axis=1)\n",
    "            std_daily_return = tf.math.reduce_std(portfolio_daily_returns, axis=1)\n",
    "\n",
    "            # แปลงเป็นค่ารายปี (Annualize)\n",
    "            annualized_return = mean_daily_return * 252.0\n",
    "            annualized_volatility = std_daily_return * tf.sqrt(252.0)\n",
    "            \n",
    "            # คำนวณ Sharpe Ratio\n",
    "            risk_free_rate = 0.02\n",
    "            sharpe = (annualized_return - risk_free_rate) / (annualized_volatility + 1e-9)\n",
    "\n",
    "            # Loss คือค่าติดลบของค่าเฉลี่ย Sharpe Ratio ของทั้ง batch\n",
    "            return -tf.reduce_mean(sharpe)\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        model.compile(loss=sharpe_loss, optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def calc_wgts(self, lkbk: int, ep: int, data_prices: pd.DataFrame, features: pd.DataFrame, \n",
    "                  forecast_horizon: int, patience=10, PE_type=pe_type, rebalance_date=None):\n",
    "        \n",
    "        if not hasattr(self, \"constraint_matrix\"):\n",
    "            self._create_constraint_matrices(data_prices.columns)\n",
    "        \n",
    "        #print(f\"\\n=== Calculating weights with lkbk={lkbk}, horizon={forecast_horizon}, ep={ep}, PE={PE_type} ===\")\n",
    "\n",
    "        # สร้างชุดข้อมูลสำหรับ training ด้วยฟังก์ชันใหม่\n",
    "        # ที่นี่ data_prices คือข้อมูลราคา, features คือข้อมูล features\n",
    "        X_train, y_train = create_direct_forecast_dataset(\n",
    "            features, \n",
    "            data_prices, \n",
    "            sequence_length=lkbk, \n",
    "            forecast_horizon=forecast_horizon\n",
    "        )\n",
    "        #print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        if len(X_train) == 0:\n",
    "            raise ValueError(\"Could not create any training samples. Check data length, lookback, and horizon.\")\n",
    "\n",
    "        #print(f\"Created training dataset: X_train shape={X_train.shape}, y_train shape={y_train.shape}\")\n",
    "        \n",
    "        # ไม่จำเป็นต้องใช้ self.data อีกต่อไป\n",
    "        # self.data = tf.cast(tf.constant(data), float) -> ลบทิ้ง\n",
    "\n",
    "        # สร้างโมเดลใหม่สำหรับทุกรอบ rebalance\n",
    "        # input_shape มาจาก (sequence_length, n_features)\n",
    "        input_shape_for_build = (X_train.shape[1], X_train.shape[2])\n",
    "        #print(f\"Input shape for model build: {input_shape_for_build}\")\n",
    "        self.model = self.build(input_shape_for_build, len(data_prices.columns), PE_type=PE_type)\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "        # ฝึกโมเดลด้วยชุดข้อมูลที่สร้างขึ้น\n",
    "        history = self.model.fit(X_train, y_train, epochs=ep, batch_size=256, \n",
    "                       shuffle=False,\n",
    "                       validation_split=0.2, \n",
    "                       callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "        # # save history to pickle file\n",
    "        # history_path = f\"{output_dir}/{train_type}/history/history_{model_type}_{pe_type}_run_{run_no}_{rebalance_date}.pkl\"\n",
    "        # with open(history_path, 'wb') as f:\n",
    "        #     pkl.dump(history.history, f)\n",
    "\n",
    "        # --- การทำนาย (Prediction) ---\n",
    "        # เราต้องใช้ข้อมูล features 'lkbk' วันล่าสุดเพื่อทำนาย weight สำหรับไตรมาสถัดไป\n",
    "        latest_features = features.iloc[-lkbk:].values\n",
    "        #print(f\"Latest features shape for prediction: {latest_features.shape}\")\n",
    "        \n",
    "        # เพิ่มมิติให้เป็น (1, lkbk, n_features) เพื่อให้ตรงกับ input ของโมเดล\n",
    "        predict_data = latest_features[np.newaxis, :, :]\n",
    "        #print(f\"Predict data shape: {predict_data.shape}\")\n",
    "\n",
    "        # ทำนายน้ำหนัก\n",
    "        raw_weights = self.model.predict(predict_data)[0]\n",
    "        #print(f\"Raw weights shape: {raw_weights.shape}\")\n",
    "        # raw_weights_non_zero_count = np.count_nonzero(raw_weights > 1e-4)\n",
    "        # print(f\"Raw Weights selected ETF: {raw_weights_non_zero_count}\")\n",
    "        \n",
    "        # ปรับน้ำหนักด้วย constraints\n",
    "        weights = self._apply_constraints_final(raw_weights)\n",
    "        #print(f\"Final weights shape: {weights.shape}\")\n",
    "        return weights, raw_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558ff619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # asset_weights = model.calc_wgts(lookback, n_epochs, train_features, train_features, patience=10)\n",
    "# def quarterly_walk_forward(df, lookback, n_epochs, features, asset_map=None, \n",
    "#                                    asset_lower=None, asset_upper=None, port_type=(0, 1), \n",
    "#                                    start_year=2020, trading_days_per_quarter=63, \n",
    "#                                    min_train_periods=252, PE_type=pe_type):\n",
    "#     \"\"\"Optimized quarterly walk-forward analysis\"\"\"\n",
    "    \n",
    "#     # Efficient data preparation\n",
    "#     original_index = df.index\n",
    "#     df_reset = df.reset_index(drop=True)\n",
    "#     features_reset = features.reset_index(drop=True)\n",
    "    \n",
    "#     # Get rebalance dates\n",
    "#     rebalance_dates = get_rebalance_dates(df, start_year)\n",
    "    \n",
    "#     # Pre-allocate result containers\n",
    "#     all_rets = []\n",
    "#     weights_list = []\n",
    "#     raw_weights_list = []\n",
    "#     rebalance_info = []\n",
    "    \n",
    "#     # Initialize model once\n",
    "#     model = Model(\n",
    "#         max_weight=1, \n",
    "#         asset_map=asset_map, \n",
    "#         asset_lower=asset_lower,\n",
    "#         asset_upper=asset_upper, \n",
    "#         port_type=port_type\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\n=== Starting Optimized Quarterly Rebalancing Analysis ===\")\n",
    "#     print(f\"Training lookback period: {lookback} days\")\n",
    "#     print(f\"Minimum training periods: {min_train_periods} days\")\n",
    "#     print(f\"Trading days per quarter: {trading_days_per_quarter} days\")\n",
    "#     print(f\"Total rebalance periods: {len(rebalance_dates)}\")\n",
    "    \n",
    "#     for i, rebalance_date in enumerate(rebalance_dates):\n",
    "#         print(f\"\\n--- Rebalancing {i+1}/{len(rebalance_dates)} ---\")\n",
    "#         print(f\"Rebalance Date: {rebalance_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "#         try:\n",
    "#             # More efficient position finding\n",
    "#             train_end_pos = original_index.get_indexer([rebalance_date], method='pad')[0] - 1\n",
    "#             if train_end_pos < 0:\n",
    "#                 continue\n",
    "                \n",
    "#         except Exception:\n",
    "#             print(f\"Cannot find position for {rebalance_date}, skipping... ❌❌❌\")\n",
    "#             continue\n",
    "        \n",
    "#         # Define training period\n",
    "#         train_start_pos = max(0, train_end_pos - min_train_periods)\n",
    "        \n",
    "#         print(f\"Training period: {original_index[train_start_pos].strftime('%Y-%m-%d')} to {original_index[train_end_pos].strftime('%Y-%m-%d')}\")\n",
    "#         print(f\"Training days: {train_end_pos - train_start_pos + 1}\")\n",
    "        \n",
    "#         # Get quarter end\n",
    "#         quarter_end = get_quarter_end_date(rebalance_date, original_index, trading_days_per_quarter)\n",
    "        \n",
    "#         try:\n",
    "#             test_end_pos = original_index.get_loc(quarter_end)\n",
    "#             print(f\"Test period: {rebalance_date.strftime('%Y-%m-%d')} to {quarter_end.strftime('%Y-%m-%d')}\")\n",
    "#             print(f\"Test days: {test_end_pos - train_end_pos}\")\n",
    "#         except Exception:\n",
    "#             print(f\"Cannot find end date for quarter, skipping... ❌❌❌\")\n",
    "#             continue\n",
    "        \n",
    "#         # Skip if insufficient future data\n",
    "#         if test_end_pos >= len(df_reset):\n",
    "#             print(f\"Not enough future data, stopping at rebalance {i+1} ❌❌❌\")\n",
    "#             break\n",
    "        \n",
    "#         # Extract and prepare training data more efficiently\n",
    "#         train_data = df_reset.iloc[train_start_pos:train_end_pos+1].copy()\n",
    "#         train_features = features_reset.iloc[train_start_pos:train_end_pos+1].copy()\n",
    "        \n",
    "#         # Efficient data cleaning\n",
    "#         train_data = train_data.ffill().bfill()\n",
    "#         train_features = train_features.fillna(0) # for cal sharpe loss (ori pct_change)\n",
    "#         # train_features_roll = train_features.rolling(window=5, min_periods=1).mean().fillna(0)\n",
    "\n",
    "#         # # # scale features - for training\n",
    "#         #sc = StandardScaler()\n",
    "#         #train_features_sc = train_features.copy()\n",
    "#         # train_features_sc = train_features_sc.rolling(window=5, min_periods=1).mean().fillna(0)\n",
    "#         #train_features_sc = pd.DataFrame(sc.fit_transform(train_features_sc), columns=train_features.columns, index=train_features.index)\n",
    "        \n",
    "#         # Calculate test returns more efficiently\n",
    "#         test_data = df_reset.iloc[train_end_pos:test_end_pos+1].copy()\n",
    "#         test_returns = test_data.pct_change().fillna(0).iloc[1:]\n",
    "\n",
    "#         print(f\"Training data shape: {train_data.shape}\")\n",
    "#         print(f\"Test returns shape: {test_returns.shape}\")\n",
    "        \n",
    "#         # Train model and get weights\n",
    "#         try:\n",
    "#             # Clear session for memory management\n",
    "#             K.clear_session()\n",
    "#             tf.keras.backend.clear_session()\n",
    "            \n",
    "#             # asset_weights = model.calc_wgts(lookback, n_epochs, train_data, train_features, patience=10)\n",
    "#             asset_weights, raw_weights = model.calc_wgts(lookback, n_epochs, train_features, train_features, patience=10, \n",
    "#                                                          PE_type=PE_type)\n",
    "#             #asset_weights, raw_weights = model.calc_wgts(lookback, n_epochs, train_features, train_features_sc, patience=10)\n",
    "\n",
    "#             print(f\"\\n{'=' * 30}\")\n",
    "#             # total weight should be 1 status\n",
    "#             total_weight = np.sum(asset_weights).round(4)\n",
    "#             if total_weight == 1:\n",
    "#                 print(f\"Total weight: {total_weight:.4f} ✅\")\n",
    "#             else:\n",
    "#                 print(f\"Total weight: {total_weight:.4f} ❌ (should be 1.0)\")\n",
    "            \n",
    "#             weights_non_zero_count = np.count_nonzero(asset_weights)\n",
    "#             print(f\"Selected ETF count: {weights_non_zero_count}\")\n",
    "#             # if weights_non_zero_count < 10 or weights_non_zero_count > 15:\n",
    "#             #     print(f\"Warning: weights have {weights_non_zero_count} ❌\")\n",
    "#             #     os.system(f'say \"Warning: Selected ETF have {weights_non_zero_count}\"')\n",
    "#             # if weights_non_zero_count >= 10 and weights_non_zero_count <= 15:\n",
    "#             #     print(f\"Selected ETF: {weights_non_zero_count} ✅\")\n",
    "\n",
    "#             # Print allocation summary\n",
    "#             if hasattr(model, 'constraint_matrix') and hasattr(model, 'asset_type_names'):\n",
    "#                 type_weights = np.dot(model.constraint_matrix, asset_weights)\n",
    "#                 print(\"Asset Type Allocation:\")\n",
    "#                 for j, (asset_type, weight) in enumerate(zip(model.asset_type_names, type_weights)):\n",
    "#                     lower = model.asset_lower.get(asset_type, 0.0)\n",
    "#                     upper = model.asset_upper.get(asset_type, 1.0)\n",
    "#                     # status\n",
    "#                     if lower <= weight.round(4) <= upper:\n",
    "#                         status = \"✅\" #Within Range\n",
    "#                     else:\n",
    "#                         status = \"❌\" #Out of Range\n",
    "#                     print(f\"  {asset_type}: {weight:.3f} ({weight*100:.2f}%) [Range: {lower:.2f}-{upper:.2f}] {status}\")\n",
    "            \n",
    "#             # Calculate out-of-sample returns\n",
    "#             if len(test_returns) > 0:\n",
    "#                 # Vectorized return calculation\n",
    "#                 oos_returns = (test_returns.values * asset_weights).sum(axis=1)\n",
    "#                 oos_returns = pd.Series(oos_returns, index=test_returns.index)\n",
    "                \n",
    "#                 all_rets.append(oos_returns)\n",
    "#                 weights_list.append(asset_weights)\n",
    "#                 raw_weights_list.append(raw_weights)\n",
    "#                 rebalance_info.append({\n",
    "#                     'rebalance_date': rebalance_date,\n",
    "#                     'quarter_end': quarter_end,\n",
    "#                     'quarter_return': oos_returns.sum(),\n",
    "#                     'quarter_days': len(oos_returns)\n",
    "#                 })\n",
    "                \n",
    "#                 print(f\"Quarter return: {oos_returns.sum():.4f} ({oos_returns.sum()*100:.2f}%)\")\n",
    "#             else:\n",
    "#                 print(\"No test returns available ❌❌❌\")\n",
    "            \n",
    "#             print(f\"\\n{'=' * 30}\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in rebalancing: {str(e)} ❌❌❌\")\n",
    "#             continue\n",
    "    \n",
    "#     # Combine results efficiently\n",
    "#     if all_rets:\n",
    "#         pnl = pd.concat(all_rets, ignore_index=False)\n",
    "#         weights_df = pd.DataFrame(weights_list, columns=df.columns)\n",
    "#         raw_weights_df = pd.DataFrame(raw_weights_list, columns=df.columns)\n",
    "#         rebalance_summary = pd.DataFrame(rebalance_info)\n",
    "        \n",
    "#         print(f\"\\n=== Rebalancing Summary ===\")\n",
    "#         print(f\"Total quarters processed: {len(rebalance_info)}\")\n",
    "#         print(f\"Total return periods: {len(pnl)}\")\n",
    "#         print(f\"Average quarterly return: {rebalance_summary['quarter_return'].mean():.4f}\")\n",
    "#         print(f\"Quarterly return std: {rebalance_summary['quarter_return'].std():.4f}\")\n",
    "        \n",
    "#         return pnl, weights_df, rebalance_summary, raw_weights_df\n",
    "#     else:\n",
    "#         print(\"No successful rebalancing periods ❌❌❌\")\n",
    "#         return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e9e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Multi-step Forecasting with Quarterly Walk-Forward Analysis\n",
    "def quarterly_walk_forward(df, lookback, n_epochs, features, asset_map=None, \n",
    "                           asset_lower=None, asset_upper=None, port_type=(0, 1), \n",
    "                           start_year=2020, trading_days_per_quarter=63, \n",
    "                           min_train_periods=252, PE_type=pe_type):\n",
    "    \n",
    "    original_index = df.index\n",
    "    df_reset = df.reset_index(drop=True)\n",
    "    features_reset = features.reset_index(drop=True)\n",
    "    \n",
    "    rebalance_dates = get_rebalance_dates(df, start_year)\n",
    "    \n",
    "    all_rets, weights_list, raw_weights_list, rebalance_info = [], [], [], []\n",
    "    \n",
    "    model = Model(\n",
    "        max_weight=1, asset_map=asset_map, asset_lower=asset_lower,\n",
    "        asset_upper=asset_upper, port_type=port_type\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Starting Quarterly Rebalancing with Direct Multi-step Forecast ===\")\n",
    "    \n",
    "    for i, rebalance_date in enumerate(rebalance_dates):\n",
    "        print(f\"\\n--- Rebalancing {i+1}/{len(rebalance_dates)} | Date: {rebalance_date.strftime('%Y-%m-%d')} ---\")\n",
    "        \n",
    "        try:\n",
    "            train_end_pos = original_index.get_indexer([rebalance_date], method='pad')[0]\n",
    "            if train_end_pos < min_train_periods:\n",
    "                print(\"Not enough historical data, skipping...\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot find position for {rebalance_date}, skipping... Error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        train_start_pos = 0 # Use all available history up to rebalance date\n",
    "        \n",
    "        print(f\"Training period: {original_index[train_start_pos].strftime('%Y-%m-%d')} to {original_index[train_end_pos-1].strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        quarter_end = get_quarter_end_date(rebalance_date, original_index, trading_days_per_quarter)\n",
    "        \n",
    "        try:\n",
    "            test_end_pos = original_index.get_loc(quarter_end)\n",
    "        except Exception:\n",
    "            print(f\"Cannot find end date for quarter, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # ใช้ df_reset และ features_reset สำหรับการ slicing ด้วย train_end_pos\n",
    "        train_data_prices = df_reset.iloc[train_start_pos:train_end_pos].copy()\n",
    "        train_features = features_reset.iloc[train_start_pos:train_end_pos].copy()\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        train_features_scaled = pd.DataFrame(scaler.fit_transform(train_features), index=train_features.index, columns=train_features.columns)\n",
    "        \n",
    "\n",
    "        test_data = df_reset.iloc[train_end_pos-1:test_end_pos].copy()\n",
    "        test_returns = test_data.pct_change().fillna(0).iloc[1:]\n",
    "\n",
    "        print(f\"Training data shape: {train_data_prices.shape}\")\n",
    "        \n",
    "        if len(test_returns) == 0:\n",
    "            print(\"No test returns available, stopping.\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            K.clear_session()\n",
    "            \n",
    "            # ส่ง forecast_horizon=trading_days_per_quarter เข้าไปใน calc_wgts\n",
    "            asset_weights, raw_weights = model.calc_wgts(\n",
    "                lkbk=lookback, \n",
    "                ep=n_epochs, \n",
    "                data_prices=train_data_prices, \n",
    "                features=train_features_scaled, # train_features, train_features_scaled\n",
    "                forecast_horizon=trading_days_per_quarter, # <--- เพิ่ม parameter นี้\n",
    "                patience=5, \n",
    "                PE_type=PE_type,\n",
    "                rebalance_date=rebalance_date.strftime('%Y-%m-%d')\n",
    "            )\n",
    "            \n",
    "            # -- ส่วนของการแสดงผลและเก็บข้อมูล (เหมือนเดิม) --\n",
    "            print(f\"\\n{'=' * 30}\")\n",
    "            # total weight should be 1 status\n",
    "            total_weight = np.sum(asset_weights).round(4)\n",
    "            if total_weight == 1:\n",
    "                print(f\"Total weight: {total_weight:.4f} ✅\")\n",
    "            else:\n",
    "                print(f\"Total weight: {total_weight:.4f} ❌ (should be 1.0)\")\n",
    "            \n",
    "            weights_non_zero_count = np.count_nonzero(asset_weights)\n",
    "            print(f\"Selected ETF count: {weights_non_zero_count}\")\n",
    "            # if weights_non_zero_count < 10 or weights_non_zero_count > 15:\n",
    "            #     print(f\"Warning: weights have {weights_non_zero_count} ❌\")\n",
    "            #     os.system(f'say \"Warning: Selected ETF have {weights_non_zero_count}\"')\n",
    "            # if weights_non_zero_count >= 10 and weights_non_zero_count <= 15:\n",
    "            #     print(f\"Selected ETF: {weights_non_zero_count} ✅\")\n",
    "\n",
    "            # Print allocation summary\n",
    "            if hasattr(model, 'constraint_matrix') and hasattr(model, 'asset_type_names'):\n",
    "                type_weights = np.dot(model.constraint_matrix, asset_weights)\n",
    "                print(\"Asset Type Allocation:\")\n",
    "                for j, (asset_type, weight) in enumerate(zip(model.asset_type_names, type_weights)):\n",
    "                    lower = model.asset_lower.get(asset_type, 0.0)\n",
    "                    upper = model.asset_upper.get(asset_type, 1.0)\n",
    "                    # status\n",
    "                    if lower <= weight.round(4) <= upper:\n",
    "                        status = \"✅\" #Within Range\n",
    "                    else:\n",
    "                        status = \"❌\" #Out of Range\n",
    "                    print(f\"  {asset_type}: {weight:.3f} ({weight*100:.2f}%) [Range: {lower:.2f}-{upper:.2f}] {status}\")\n",
    "            \n",
    "            # Calculate out-of-sample returns\n",
    "            if len(test_returns) > 0:\n",
    "                # Vectorized return calculation\n",
    "                oos_returns = (test_returns.values * asset_weights).sum(axis=1)\n",
    "                oos_returns = pd.Series(oos_returns, index=test_returns.index)\n",
    "                \n",
    "                all_rets.append(oos_returns)\n",
    "                weights_list.append(asset_weights)\n",
    "                raw_weights_list.append(raw_weights)\n",
    "                rebalance_info.append({\n",
    "                    'rebalance_date': rebalance_date,\n",
    "                    'quarter_end': quarter_end,\n",
    "                    'quarter_return': oos_returns.sum(),\n",
    "                    'quarter_days': len(oos_returns)\n",
    "                })\n",
    "                \n",
    "                print(f\"Quarter return: {oos_returns.sum():.4f} ({oos_returns.sum()*100:.2f}%)\")\n",
    "            else:\n",
    "                print(\"No test returns available ❌❌❌\")\n",
    "            \n",
    "            print(f\"\\n{'=' * 30}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in rebalancing: {str(e)} ❌❌❌\")\n",
    "            continue\n",
    "    \n",
    "    # Combine results efficiently\n",
    "    if all_rets:\n",
    "        pnl = pd.concat(all_rets, ignore_index=False)\n",
    "        weights_df = pd.DataFrame(weights_list, columns=df.columns)\n",
    "        raw_weights_df = pd.DataFrame(raw_weights_list, columns=df.columns)\n",
    "        rebalance_summary = pd.DataFrame(rebalance_info)\n",
    "        \n",
    "        print(f\"\\n=== Rebalancing Summary ===\")\n",
    "        print(f\"Total quarters processed: {len(rebalance_info)}\")\n",
    "        print(f\"Total return periods: {len(pnl)}\")\n",
    "        print(f\"Average quarterly return: {rebalance_summary['quarter_return'].mean():.4f}\")\n",
    "        print(f\"Quarterly return std: {rebalance_summary['quarter_return'].std():.4f}\")\n",
    "        \n",
    "        return pnl, weights_df, rebalance_summary, raw_weights_df\n",
    "    else:\n",
    "        print(\"No successful rebalancing periods ❌❌❌\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookback period: 252 days\n",
      "Trading days per quarter: 63 days\n",
      "Minimum training periods: 756 days\n",
      "Rebalance Dates: ['2020-01-02', '2020-04-01', '2020-07-01', '2020-10-01', '2021-01-04', '2021-04-01', '2021-07-01', '2021-10-01', '2022-01-03', '2022-04-01', '2022-07-01', '2022-10-03', '2023-01-03', '2023-04-03', '2023-07-03', '2023-10-02', '2024-01-02', '2024-04-01', '2024-07-01', '2024-10-01']\n",
      "\n",
      "=== Starting Quarterly Rebalancing with Direct Multi-step Forecast ===\n",
      "\n",
      "--- Rebalancing 1/20 | Date: 2020-01-02 ---\n",
      "Training period: 2015-01-02 to 2019-12-31\n",
      "Training data shape: (1258, 33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 10:36:59.413803: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4\n",
      "2025-06-30 10:36:59.413837: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-06-30 10:36:59.413844: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-06-30 10:36:59.413860: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-30 10:36:59.413871: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-06-30 10:37:03.200064: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "data_train = data.copy()\n",
    "features = data_train.pct_change().fillna(0)  # Calculate percentage change for features\n",
    "\n",
    "# Reset states generated by Keras\n",
    "K.clear_session()\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "n_lookback = avg_days * 12 # Sequence: โค้ดจะสร้าง Input โดยสำหรับ ทุกๆ วัน ในชุดข้อมูลเทรน มันจะ \"มองย้อนกลับไป\" เป็นจำนวน n_lookback วัน (เช่น 21 วันทำการ หรือประมาณ 1 เดือน)\n",
    "n_trading_days_per_quarter = avg_days * 3  # 21 days/month * 3 months = 63 days\n",
    "n_min_train_periods = avg_days * 12 * 3  # 3 years of training data = 252*3 days = 756 days\n",
    "\n",
    "print(f\"Lookback period: {n_lookback} days\")\n",
    "print(f\"Trading days per quarter: {n_trading_days_per_quarter} days\")\n",
    "print(f\"Minimum training periods: {n_min_train_periods} days\")\n",
    "\n",
    "# Run quarterly rebalancing with constraints\n",
    "pnl, model_weights, rebalance_summary, Model_raw_weights = quarterly_walk_forward(\n",
    "    data_train, \n",
    "    lookback=n_lookback, \n",
    "    n_epochs=1, \n",
    "    features=features.fillna(0),\n",
    "    asset_map=asset_map, \n",
    "    asset_lower=asset_lower_aggressive,\n",
    "    asset_upper=asset_upper_aggressive,\n",
    "    port_type=long_only,\n",
    "    start_year=start_rebalance_year,\n",
    "    trading_days_per_quarter=n_trading_days_per_quarter,  \n",
    "    min_train_periods=n_min_train_periods,\n",
    "    PE_type=pe_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_dates =  get_rebalance_dates(data, start_year=start_rebalance_year)\n",
    "\n",
    "# add rebalance_dates to weights_df index\n",
    "weights_df = model_weights.copy()\n",
    "weights_df.index = rebalance_dates[:len(weights_df)]\n",
    "print(\"\\n%Weights DataFrame with Rebalance Dates:\")\n",
    "display(weights_df.multiply(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7672ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weights_df = Model_raw_weights.copy()\n",
    "raw_weights_df.index = rebalance_dates[:len(raw_weights_df)]\n",
    "print(\"\\n%Raw Weights DataFrame with Rebalance Dates:\")\n",
    "display(raw_weights_df.multiply(100).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the Check and Print Results ---\n",
    "violations_found = check_portfolio_constraints(\n",
    "    weights_df, \n",
    "    asset_map, \n",
    "    asset_lower_aggressive, \n",
    "    asset_upper_aggressive\n",
    ")\n",
    "\n",
    "if not violations_found:\n",
    "    print(\"✅ All portfolio weights satisfy the constraints.\")\n",
    "else:\n",
    "    print(\"❌ Constraint violations were found:\")\n",
    "    for date, messages in violations_found.items():\n",
    "        print(f\"\\nOn {date}:\")\n",
    "        for msg in messages:\n",
    "            print(f\"  - {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d678ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_df.to_csv(f'{output_dir}/{train_type}/raw_weights_df_run{run_no}.csv', index=True)\n",
    "save_dataframe_to_new_sheet(weights_df, results_excel_path, 'Raw Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3072e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('say \"Model training has finished\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fe4b4",
   "metadata": {},
   "source": [
    "# Compared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d1de2",
   "metadata": {},
   "source": [
    "### 1. Model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86527557",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_df = weights_df.copy()\n",
    "model_weights_df = model_weights_df.round(4)\n",
    "model_weights_df = model_weights_df.abs()\n",
    "save_dataframe_to_new_sheet(model_weights_df, results_excel_path, 'Model Weights')\n",
    "# model_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89422987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of weights > 0 in each row\n",
    "non_zero_weights_count = (model_weights_df > 0).sum(axis=1)\n",
    "print(\"\\nNumber of non-zero weights in each row:\")\n",
    "print(non_zero_weights_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e1f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total weight in each row to ensure it sums to 1\n",
    "total_weights = model_weights_df.sum(axis=1).round(2)\n",
    "print(\"\\nTotal weights in each row (should be 1):\")\n",
    "print(total_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7c7e1",
   "metadata": {},
   "source": [
    "### 2. Traditional mean-variance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ea45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free = 0.02  # Example risk-free rate\n",
    "mvo_weights_df = mvo_quarterly_rebalancing(data, asset_map, \n",
    "                                           asset_lower_aggressive, asset_upper_aggressive, \n",
    "                                           port_type, start_year=start_rebalance_year,\n",
    "                                           trading_days_per_quarter=n_trading_days_per_quarter,\n",
    "                                           min_train_periods=n_min_train_periods,\n",
    "                                           risk_free_rate=risk_free)\n",
    "\n",
    "mvo_weights_df = mvo_weights_df.round(4)\n",
    "save_dataframe_to_new_sheet(mvo_weights_df, results_excel_path, 'MVO Weights')\n",
    "# mvo_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1996f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the Check and Print Results ---\n",
    "violations_found = check_portfolio_constraints(\n",
    "    mvo_weights_df, \n",
    "    asset_map, \n",
    "    asset_lower_aggressive, \n",
    "    asset_upper_aggressive\n",
    ")\n",
    "\n",
    "if not violations_found:\n",
    "    print(\"✅ All portfolio weights satisfy the constraints.\")\n",
    "else:\n",
    "    print(\"❌ Constraint violations were found:\")\n",
    "    for date, messages in violations_found.items():\n",
    "        print(f\"\\nOn {date}:\")\n",
    "        for msg in messages:\n",
    "            print(f\"  - {msg}\")\n",
    "\n",
    "# check number of weights > 0 in each row\n",
    "non_zero_weights_count = (mvo_weights_df > 0).sum(axis=1)\n",
    "print(\"\\nNumber of non-zero weights in each row:\")\n",
    "print(non_zero_weights_count)\n",
    "\n",
    "# check total weight in each row to ensure it sums to 1\n",
    "total_weights = mvo_weights_df.sum(axis=1).round(2)\n",
    "print(\"\\nTotal weights in each row (should be 1):\")\n",
    "print(total_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a622345",
   "metadata": {},
   "source": [
    "### 3. Equal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1047e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run equal weight portfolio\n",
    "equal_weights_df = equal_weight_portfolio(data, rebalance_dates)\n",
    "equal_weights_df = equal_weights_df.round(4)\n",
    "save_dataframe_to_new_sheet(equal_weights_df, results_excel_path, 'Equal Weights')\n",
    "# equal_weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263dcf5d",
   "metadata": {},
   "source": [
    "### 4. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ed02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_df = pd.DataFrame(\n",
    "   index=rebalance_dates,\n",
    "   columns=data.columns,\n",
    "   data=0.0\n",
    ")\n",
    "num_cash = sum(1 for v in asset_map.values() if v == 'Cash_Equivalent')\n",
    "num_fixed_income = sum(1 for v in asset_map.values() if v == 'Fixed_Income')\n",
    "num_alternatives = sum(1 for v in asset_map.values() if v == 'Alternatives')\n",
    "print(f\"Number of Cash Equivalents: {num_cash}\")\n",
    "print(f\"Number of Fixed Income: {num_fixed_income}\")\n",
    "print(f\"Number of Alternatives: {num_alternatives}\")\n",
    "\n",
    "benchmark_df['SHV'] = 0.05 / num_cash\n",
    "benchmark_df[['BND', 'BNDX', 'JNK']] = 0.05 / num_fixed_income\n",
    "benchmark_df['VT'] = 0.75\n",
    "benchmark_df[['REET', 'IGF', 'PDBC', 'GLD']] = 0.15 / num_alternatives\n",
    "save_dataframe_to_new_sheet(benchmark_df, results_excel_path, 'Beanchmark Weights')\n",
    "# benchmark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb944a61",
   "metadata": {},
   "source": [
    "### 4. Compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b19de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main analysis\n",
    "start_date = rebalance_dates[0]  # First rebalance date\n",
    "\n",
    "# Calculate portfolio returns for each strategy\n",
    "portfolios = {\n",
    "    'Model Portfolio': model_weights_df,\n",
    "    'MVO Portfolio': mvo_weights_df,\n",
    "    'Equal Weight': equal_weights_df,\n",
    "    'Benchmark': benchmark_df\n",
    "}\n",
    "\n",
    "portfolio_returns = {}\n",
    "for name, weights in portfolios.items():\n",
    "    returns = calculate_portfolio_returns(data, weights, rebalance_dates, start_date)\n",
    "    portfolio_returns[name] = returns\n",
    "\n",
    "# Calculate performance metrics\n",
    "performance_metrics = {}\n",
    "benchmark_returns = portfolio_returns['Benchmark']\n",
    "\n",
    "for name, returns in portfolio_returns.items():\n",
    "    if name == 'Benchmark':\n",
    "        metrics = calculate_performance_metrics(returns, returns)  # Self as benchmark\n",
    "    else:\n",
    "        metrics = calculate_performance_metrics(returns, benchmark_returns)\n",
    "    performance_metrics[name] = metrics\n",
    "\n",
    "# Create performance comparison DataFrame\n",
    "performance_df = pd.DataFrame(performance_metrics).T\n",
    "print(\"Portfolio Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "# performance_df.to_csv(f'{output_dir}/{train_type}/performance_comparison_run{run_no}.csv')\n",
    "save_dataframe_to_new_sheet(performance_df.T, results_excel_path, 'Performance Comparison')\n",
    "performance_df.round(4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75167f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=('Cumulative Returns', 'Maximum Drawdown'),\n",
    "    vertical_spacing=0.12,\n",
    "    row_heights=[0.7, 0.3]\n",
    ")\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Plot cumulative returns\n",
    "for i, (name, returns) in enumerate(portfolio_returns.items()):\n",
    "    #if name != 'Benchmark':\n",
    "    cumulative_returns = (1 + returns).cumprod()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=cumulative_returns.index,\n",
    "            y=cumulative_returns.values,\n",
    "            mode='lines',\n",
    "            name=name,\n",
    "            line=dict(color=colors[i], width=2),\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Plot drawdowns\n",
    "for i, (name, returns) in enumerate(portfolio_returns.items()):\n",
    "#if name != 'Benchmark':\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    rolling_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - rolling_max) / rolling_max * 100\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=drawdown.index,\n",
    "            y=drawdown.values,\n",
    "            mode='lines',\n",
    "            name=name,\n",
    "            line=dict(color=colors[i], width=2),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Portfolio Performance Comparison',\n",
    "    height=800,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Cumulative Return\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Drawdown (%)\", row=2, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the plot to a file\n",
    "fig.write_html(f'{output_dir}/Others/{model_type}/{train_type}/portfolio_performance_{model_type}_run_{run_no}.html')\n",
    "# save image to file\n",
    "fig.write_image(f'{output_dir}/Others/{model_type}/{train_type}/portfolio_performance_{model_type}_run_{run_no}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nKey Performance Highlights:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "best_return = performance_df['Annualized Return (%)'].idxmax()\n",
    "print(f\"Best Annualized Return: {best_return} ({performance_df.loc[best_return, 'Annualized Return (%)']:.2f}%)\")\n",
    "\n",
    "best_sharpe = performance_df['Sharpe Ratio'].idxmax()\n",
    "print(f\"Best Sharpe Ratio: {best_sharpe} ({performance_df.loc[best_sharpe, 'Sharpe Ratio']:.3f})\")\n",
    "\n",
    "best_sortino = performance_df['Sortino Ratio'].idxmax()\n",
    "print(f\"Best Sortino Ratio: {best_sortino} ({performance_df.loc[best_sortino, 'Sortino Ratio']:.3f})\")\n",
    "\n",
    "lowest_dd = performance_df['Max Drawdown (%)'].idxmax()  # Most negative (lowest)\n",
    "print(f\"Lowest Max Drawdown: {lowest_dd} ({performance_df.loc[lowest_dd, 'Max Drawdown (%)']:.2f}%)\")\n",
    "\n",
    "lowest_vol = performance_df['Volatility (%)'].idxmin()\n",
    "print(f\"Lowest Volatility: {lowest_vol} ({performance_df.loc[lowest_vol, 'Volatility (%)']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('say \"Model comparison has finished\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34cafe",
   "metadata": {},
   "source": [
    "# Quarterly Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quarterly periods\n",
    "quarterly_periods = get_quarterly_periods(rebalance_dates, data)\n",
    "quarterly_periods\n",
    "\n",
    "# Portfolio definitions\n",
    "portfolios = {\n",
    "    'Model Portfolio': model_weights_df,\n",
    "    'MVO Portfolio': mvo_weights_df,\n",
    "    'Equal Weight': equal_weights_df,\n",
    "    'Benchmark': benchmark_df\n",
    "}\n",
    "\n",
    "# Calculate quarterly performance for each portfolio\n",
    "quarterly_results = {}\n",
    "weights_df_2 = weights_df.copy\n",
    "for portfolio_name, weights_df_2 in portfolios.items():\n",
    "    quarterly_results[portfolio_name] = {}\n",
    "    \n",
    "    for period in quarterly_periods:\n",
    "        quarter = period['quarter']\n",
    "        returns = calculate_quarterly_portfolio_returns(data, weights_df_2, period)\n",
    "        metrics = calculate_quarterly_metrics(returns)\n",
    "        quarterly_results[portfolio_name][quarter] = metrics\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "all_metrics = ['Total Return (%)', 'Annualized Return (%)',\n",
    "               'Volatility (%)', 'Max Drawdown (%)', \n",
    "               'Max Drawdown Duration (days)', 'Sharpe Ratio', 'Sortino Ratio']\n",
    "quarterly_comparison = {}\n",
    "\n",
    "for metric in all_metrics:\n",
    "    quarterly_comparison[metric] = pd.DataFrame({\n",
    "        portfolio: {quarter: quarterly_results[portfolio][quarter][metric] \n",
    "                   for quarter in quarterly_results[portfolio]}\n",
    "        for portfolio in portfolios.keys()\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for metric in all_metrics:\n",
    "    print(f\"\\n{metric}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(quarterly_comparison[metric].round(4))\n",
    "    # quarterly_comparison[metric].to_csv(f'{output_dir}/{train_type}/quarterly_{metric.lower().replace(\" \", \"_\")}_run{run_no}.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e561ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- เริ่มโค้ดสำหรับบันทึก Excel ---\n",
    "qoq_excel_path = f\"{output_dir}/Others/{model_type}/{train_type}/02_Quarterly_{model_type}_{pe_type}_{pre_post}_run_{run_no}.xlsx\"\n",
    "\n",
    "for metric in all_metrics:\n",
    "    sheet_name = metric.replace(\" \", \"_\")\n",
    "    df_to_save = quarterly_comparison[metric].round(4)\n",
    "    save_dataframe_to_new_sheet(df_to_save, qoq_excel_path, sheet_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Quarterly Returns (%)', 'Quarterly Volatility (%)', \n",
    "                   'Quarterly Sharpe Ratio', 'Quarterly Max Drawdown (%)'),\n",
    "    vertical_spacing=0.2,\n",
    "    horizontal_spacing=0.2\n",
    ")\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "quarters = list(quarterly_comparison['Total Return (%)'].index)\n",
    "\n",
    "# Plot quarterly returns\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=quarters,\n",
    "            y=quarterly_comparison['Total Return (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=quarters,\n",
    "            y=quarterly_comparison['Volatility (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=quarters,\n",
    "            y=quarterly_comparison['Sharpe Ratio'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=quarters,\n",
    "            y=quarterly_comparison['Max Drawdown (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Quarterly Portfolio Performance Comparison',\n",
    "    height=700,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig.update_yaxes(title_text=\"Return (%)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Volatility (%)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Sharpe Ratio\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Max Drawdown (%)\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Summary Statistics by Quarter\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUARTERLY PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for quarter in quarters:\n",
    "    print(f\"\\n{quarter}:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Best performing portfolio this quarter\n",
    "    quarter_returns = quarterly_comparison['Total Return (%)'].loc[quarter]\n",
    "    best_return = quarter_returns.idxmax()\n",
    "    print(f\"Best Return: {best_return} ({quarter_returns[best_return]:.2f}%)\")\n",
    "    \n",
    "    # Best Sharpe ratio this quarter\n",
    "    quarter_sharpe = quarterly_comparison['Sharpe Ratio'].loc[quarter].dropna()\n",
    "    if not quarter_sharpe.empty:\n",
    "        best_sharpe = quarter_sharpe.idxmax()\n",
    "        print(f\"Best Sharpe: {best_sharpe} ({quarter_sharpe[best_sharpe]:.3f})\")\n",
    "    \n",
    "    # Lowest volatility this quarter\n",
    "    quarter_vol = quarterly_comparison['Volatility (%)'].loc[quarter].dropna()\n",
    "    if not quarter_vol.empty:\n",
    "        lowest_vol = quarter_vol.idxmin()\n",
    "        print(f\"Lowest Vol: {lowest_vol} ({quarter_vol[lowest_vol]:.2f}%)\")\n",
    "\n",
    "# Ranking Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PORTFOLIO RANKINGS BY QUARTER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ranking_df = pd.DataFrame(index=quarters, columns=portfolios.keys())\n",
    "\n",
    "for quarter in quarters:\n",
    "    # Rank by quarterly returns (1 = best)\n",
    "    quarter_returns = quarterly_comparison['Total Return (%)'].loc[quarter]\n",
    "    ranks = quarter_returns.rank(ascending=False, method='min')\n",
    "    ranking_df.loc[quarter] = ranks\n",
    "\n",
    "print(\"\\nRanking by Quarterly Returns (1=Best, 4=Worst):\")\n",
    "print(ranking_df.astype(int))\n",
    "\n",
    "# Average ranking\n",
    "avg_ranking = ranking_df.mean().sort_values()\n",
    "print(f\"\\nAverage Ranking Across All Quarters:\")\n",
    "print(\"-\" * 40)\n",
    "for portfolio, avg_rank in avg_ranking.items():\n",
    "    print(f\"{portfolio}: {avg_rank:.2f}\")\n",
    "\n",
    "# Win rate analysis\n",
    "print(f\"\\nQuarterly Win Rate (% of quarters ranked #1):\")\n",
    "print(\"-\" * 45)\n",
    "for portfolio in portfolios.keys():\n",
    "    win_rate = (ranking_df[portfolio] == 1).sum() / len(quarters) * 100\n",
    "    print(f\"{portfolio}: {win_rate:.1f}%\")\n",
    "\n",
    "# Save the plot to a file\n",
    "fig.write_html(f'{output_dir}/Others/{model_type}/{train_type}/QoQ_performance_{model_type}_run_{run_no}.html')\n",
    "fig.write_image(f'{output_dir}/Others/{model_type}/{train_type}/QoQ_performance_{model_type}_run_{run_no}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf5028f",
   "metadata": {},
   "source": [
    "### Yearly Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b51084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Analysis\n",
    "print(\"Yearly Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get yearly periods\n",
    "yearly_periods = get_yearly_periods(rebalance_dates, data)\n",
    "\n",
    "# Portfolio definitions\n",
    "portfolios = {\n",
    "    'Model Portfolio': model_weights_df,\n",
    "    'MVO Portfolio': mvo_weights_df,\n",
    "    'Equal Weight': equal_weights_df,\n",
    "    'Benchmark': benchmark_df\n",
    "}\n",
    "\n",
    "# Calculate yearly performance for each portfolio\n",
    "yearly_results = {}\n",
    "weights_df_2 = weights_df.copy()\n",
    "for portfolio_name, weights_df_2 in portfolios.items():\n",
    "    yearly_results[portfolio_name] = {}\n",
    "    \n",
    "    for period in yearly_periods:\n",
    "        year = period['year']\n",
    "        returns = calculate_yearly_portfolio_returns(data, weights_df_2, period, rebalance_dates)\n",
    "        metrics = calculate_yearly_metrics(returns)\n",
    "        yearly_results[portfolio_name][year] = metrics\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "all_metrics = ['Total Return (%)', 'Annualized Return (%)', 'Volatility (%)', \n",
    "               'Sharpe Ratio', 'Sortino Ratio', 'Max Drawdown (%)', \n",
    "               'Max DD Duration (days)', 'VaR 95% (%)', 'Calmar Ratio', 'Trading Days']\n",
    "\n",
    "yearly_comparison = {}\n",
    "\n",
    "for metric in all_metrics:\n",
    "    yearly_comparison[metric] = pd.DataFrame({\n",
    "        portfolio: {year: yearly_results[portfolio][year][metric] \n",
    "                   for year in yearly_results[portfolio]}\n",
    "        for portfolio in portfolios.keys()\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for metric in all_metrics:\n",
    "    print(f\"\\n{metric}\")\n",
    "    print(\"-\" * 40)\n",
    "    if metric in ['Trading Days']:\n",
    "        print(yearly_comparison[metric].astype(int))\n",
    "    else:\n",
    "        print(yearly_comparison[metric].round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Annual Returns (%)', 'Annual Volatility (%)', \n",
    "                   'Annual Sharpe Ratio', 'Annual Max Drawdown (%)'),\n",
    "                   #'Annual Sortino Ratio', 'Annual Calmar Ratio'),\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "years = list(yearly_comparison['Total Return (%)'].index)\n",
    "\n",
    "# Plot annual returns\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=years,\n",
    "            y=yearly_comparison['Total Return (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Plot annual volatility\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=years,\n",
    "            y=yearly_comparison['Volatility (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Plot annual Sharpe ratio\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=years,\n",
    "            y=yearly_comparison['Sharpe Ratio'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Plot annual max drawdown\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=years,\n",
    "            y=yearly_comparison['Max Drawdown (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# # Plot annual Sortino ratio\n",
    "# for i, portfolio in enumerate(portfolios.keys()):\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=years,\n",
    "#             y=yearly_comparison['Sortino Ratio'][portfolio],\n",
    "#             mode='lines+markers',\n",
    "#             name=portfolio,\n",
    "#             line=dict(color=colors[i]),\n",
    "#             showlegend=False\n",
    "#         ),\n",
    "#         row=3, col=1\n",
    "#     )\n",
    "\n",
    "# # Plot annual Calmar ratio\n",
    "# for i, portfolio in enumerate(portfolios.keys()):\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=years,\n",
    "#             y=yearly_comparison['Calmar Ratio'][portfolio],\n",
    "#             mode='lines+markers',\n",
    "#             name=portfolio,\n",
    "#             line=dict(color=colors[i]),\n",
    "#             showlegend=False\n",
    "#         ),\n",
    "#         row=3, col=2\n",
    "#     )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Annual Portfolio Performance Comparison',\n",
    "    height=900,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig.update_yaxes(title_text=\"Return (%)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Volatility (%)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Sharpe Ratio\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Max Drawdown (%)\", row=2, col=2)\n",
    "# fig.update_yaxes(title_text=\"Sortino Ratio\", row=3, col=1)\n",
    "# fig.update_yaxes(title_text=\"Calmar Ratio\", row=3, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the plot to a file\n",
    "fig.write_html(f'{output_dir}/Others/{model_type}/{train_type}/YoY_performance_{model_type}_run_{run_no}.html')\n",
    "fig.write_image(f'{output_dir}/Others/{model_type}/{train_type}/YoY_performance_{model_type}_run_{run_no}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b74cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics by Year\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANNUAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for year in years:\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Best performing portfolio this year\n",
    "    year_returns = yearly_comparison['Total Return (%)'].loc[year]\n",
    "    best_return = year_returns.idxmax()\n",
    "    print(f\"Best Return: {best_return} ({year_returns[best_return]:.2f}%)\")\n",
    "    \n",
    "    # Best Sharpe ratio this year\n",
    "    year_sharpe = yearly_comparison['Sharpe Ratio'].loc[year].dropna()\n",
    "    if not year_sharpe.empty:\n",
    "        best_sharpe = year_sharpe.idxmax()\n",
    "        print(f\"Best Sharpe: {best_sharpe} ({year_sharpe[best_sharpe]:.3f})\")\n",
    "    \n",
    "    # Lowest volatility this year\n",
    "    year_vol = yearly_comparison['Volatility (%)'].loc[year].dropna()\n",
    "    if not year_vol.empty:\n",
    "        lowest_vol = year_vol.idxmin()\n",
    "        print(f\"Lowest Vol: {lowest_vol} ({year_vol[lowest_vol]:.2f}%)\")\n",
    "    \n",
    "    # Best Calmar ratio this year\n",
    "    year_calmar = yearly_comparison['Calmar Ratio'].loc[year].dropna()\n",
    "    if not year_calmar.empty:\n",
    "        best_calmar = year_calmar.idxmax()\n",
    "        print(f\"Best Calmar: {best_calmar} ({year_calmar[best_calmar]:.3f})\")\n",
    "\n",
    "# Ranking Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PORTFOLIO RANKINGS BY YEAR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ranking_df = pd.DataFrame(index=years, columns=portfolios.keys())\n",
    "\n",
    "for year in years:\n",
    "    # Rank by annual returns (1 = best)\n",
    "    year_returns = yearly_comparison['Total Return (%)'].loc[year]\n",
    "    ranks = year_returns.rank(ascending=False, method='min')\n",
    "    ranking_df.loc[year] = ranks\n",
    "\n",
    "print(\"\\nRanking by Annual Returns (1=Best, 4=Worst):\")\n",
    "print(ranking_df.astype(int))\n",
    "\n",
    "# Average ranking\n",
    "avg_ranking = ranking_df.mean().sort_values()\n",
    "print(f\"\\nAverage Ranking Across All Years:\")\n",
    "print(\"-\" * 40)\n",
    "for portfolio, avg_rank in avg_ranking.items():\n",
    "    print(f\"{portfolio}: {avg_rank:.2f}\")\n",
    "\n",
    "# Win rate analysis\n",
    "print(f\"\\nAnnual Win Rate (% of years ranked #1):\")\n",
    "print(\"-\" * 45)\n",
    "for portfolio in portfolios.keys():\n",
    "    win_rate = (ranking_df[portfolio] == 1).sum() / len(years) * 100\n",
    "    print(f\"{portfolio}: {win_rate:.1f}%\")\n",
    "\n",
    "# Multi-year consistency analysis\n",
    "print(f\"\\nConsistency Analysis:\")\n",
    "print(\"-\" * 25)\n",
    "for portfolio in portfolios.keys():\n",
    "    returns_series = yearly_comparison['Total Return (%)'][portfolio].dropna()\n",
    "    if len(returns_series) > 1:\n",
    "        consistency = returns_series.std()\n",
    "        print(f\"{portfolio} - Return Std Dev: {consistency:.2f}%\")\n",
    "\n",
    "# Best and worst years\n",
    "print(f\"\\nBest and Worst Years:\")\n",
    "print(\"-\" * 25)\n",
    "for portfolio in portfolios.keys():\n",
    "    returns_series = yearly_comparison['Total Return (%)'][portfolio].dropna()\n",
    "    if len(returns_series) > 0:\n",
    "        best_year = returns_series.idxmax()\n",
    "        worst_year = returns_series.idxmin()\n",
    "        print(f\"{portfolio}:\")\n",
    "        print(f\"  Best: {best_year} ({returns_series[best_year]:.2f}%)\")\n",
    "        print(f\"  Worst: {worst_year} ({returns_series[worst_year]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- เริ่มโค้ดสำหรับบันทึก Excel ---\n",
    "output_excel_path = f\"{output_dir}/Others/{model_type}/{train_type}/03_Yearly_{model_type}_{pe_type}_{pre_post}_run_{run_no}.xlsx\"\n",
    "\n",
    "for metric in all_metrics:\n",
    "    sheet_name = metric.replace(\" \", \"_\")\n",
    "    df_to_save = yearly_comparison[metric].round(4)\n",
    "    save_dataframe_to_new_sheet(df_to_save, output_excel_path, sheet_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04230e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_comparison['Sortino Ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d858fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('say \"All code has finished\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
