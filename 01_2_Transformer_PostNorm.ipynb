{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c039ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from Util_def import *\n",
    "from Util_model import *\n",
    "from pypfopt import (\n",
    "    EfficientFrontier,\n",
    "    risk_models,\n",
    "    expected_returns,\n",
    "    objective_functions,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETF_list = [\n",
    "    'SHV',\n",
    "    'BND', 'BNDX', 'JNK',\n",
    "    'VT', 'VEA', 'IEMG',\n",
    "    'VOO', 'QQQ', 'DIA', 'VGK', 'EWJ', 'MCHI', 'THD', 'VNM', 'INDA',\n",
    "    'RXI', 'KXI', 'IXC', 'IXG', 'IXJ', 'EXI', 'IXN', 'IXP', 'JXI',\n",
    "    'ITA', 'ICLN', 'SKYY', 'SMH',\n",
    "    'REET', 'IGF', 'PDBC', 'GLD'\n",
    "]\n",
    "\n",
    "# 5 years data\n",
    "startDate = dt.datetime(2015, 1, 1)\n",
    "endDate = dt.datetime(2025, 7, 28)\n",
    "\n",
    "start_rebalance_year = 2020  # startDate.year + 3\n",
    "\n",
    "data = getData(ETF_list, startDate, endDate)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "# data.fillna(method='bfill', inplace=True)\n",
    "print(data.info())\n",
    "avg_days = avg_days_per_month(data)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Min Date:\", data.index.min())\n",
    "print(\"Max Date:\", data.index.max())\n",
    "print(\"Start Rebalance Year:\", start_rebalance_year)\n",
    "print(f\"Average number of trading days per month: {avg_days}\", \"days\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Portfolio Type ######\n",
    "long_only = tuple([0,1])\n",
    "long_short = tuple([-1,1])\n",
    "\n",
    "port_type = long_only \n",
    "\n",
    "###### Adding Constraints ######\n",
    "# Asset Mapping\n",
    "asset_map = {\n",
    "    'SHV': 'Cash_Equivalent',\n",
    "    \n",
    "    'BND': 'Fixed_Income',\n",
    "    'BNDX': 'Fixed_Income',\n",
    "    'JNK': 'Fixed_Income',\n",
    "\n",
    "    'VT': 'Equity',\n",
    "    'VEA': 'Equity',\n",
    "    'IEMG': 'Equity',\n",
    "\n",
    "    'VOO': 'Equity',\n",
    "    'QQQ': 'Equity',\n",
    "    'DIA': 'Equity',\n",
    "    'VGK': 'Equity',\n",
    "    'EWJ': 'Equity',\n",
    "    'MCHI': 'Equity',\n",
    "    'THD': 'Equity',\n",
    "    'VNM': 'Equity',\n",
    "    'INDA': 'Equity',\n",
    "\n",
    "    'RXI': 'Equity',\n",
    "    'KXI': 'Equity',\n",
    "    'IXC': 'Equity',\n",
    "    'IXG': 'Equity',\n",
    "    'IXJ': 'Equity',\n",
    "    'EXI': 'Equity',\n",
    "    'IXN': 'Equity',\n",
    "    'IXP': 'Equity',\n",
    "    'JXI': 'Equity',\n",
    "\n",
    "    'ITA': 'Equity',\n",
    "    'ICLN': 'Equity',\n",
    "    'SKYY': 'Equity',\n",
    "    'SMH': 'Equity',\n",
    "\n",
    "    'REET': 'Alternatives',\n",
    "    'IGF': 'Alternatives',\n",
    "    'PDBC': 'Alternatives',\n",
    "    'GLD': 'Alternatives',\n",
    "}\n",
    "\n",
    "### Aggressive Portfolio ###\n",
    "asset_lower_aggressive = {\n",
    "    'Cash_Equivalent': 0.0,\n",
    "    'Fixed_Income': 0.0,\n",
    "    'Equity': 0.55,\n",
    "    'Alternatives': 0.0}\n",
    "asset_upper_aggressive = {\n",
    "    'Cash_Equivalent': 0.4,\n",
    "    'Fixed_Income': 0.3,\n",
    "    'Equity': 0.9,\n",
    "    'Alternatives': 0.3}\n",
    "\n",
    "len(ETF_list), len(asset_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11536836",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the results\n",
    "output_dir = 'Results'\n",
    "model_type = 'Transformer'  # 'Transformer' or 'LSTM'\n",
    "pe_type = 'tAPE'          \n",
    "# 'OriPE', 'Time2Vec', \n",
    "# 'ConvSPE', 'SineSPE', \n",
    "# 'TemporalPE', 'LearnablePE', \n",
    "# 'AbsolutePE', 'tAPE'\n",
    "pre_post = 'PostNorm'       # 'PostNorm' or 'PreNorm'\n",
    "n_temp = 1.0\n",
    "train_type = f'01_2_{model_type}_{pe_type}_{pre_post}_temp_{n_temp}'  # '01_1', '01_2', '01_3', etc.\n",
    "run_no = 3\n",
    "\n",
    "# --- เริ่มโค้ดสำหรับบันทึก Excel ---\n",
    "results_excel_path = f\"{output_dir}/{train_type}/01_Results_{model_type}_{pe_type}_{pre_post}_run_{run_no}.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe7b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, LayerNormalization, MultiHeadAttention, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from dateutil.parser import parse\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.models import Model as KModel, Sequential\n",
    "from tensorflow.keras import layers, Model as KModel\n",
    "import cvxpy as cp\n",
    "import cvxopt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  details = tf.config.experimental.get_device_details(gpus[0])\n",
    "  print(\"GPU details: \", details)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2a2dd",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, max_weight=1, asset_map=None, asset_lower=None, asset_upper=None, port_type=None):\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "        self.max_weight = max_weight\n",
    "        self.asset_map = asset_map or {}\n",
    "        self.asset_lower = asset_lower or {}\n",
    "        self.asset_upper = asset_upper or {}\n",
    "        self.port_type = port_type\n",
    "        self.asset_columns = None\n",
    "\n",
    "    def _create_constraint_matrices(self, columns):\n",
    "        \"\"\"Create constraint matrices for asset‐type bounds.\"\"\"\n",
    "        self.asset_columns = columns\n",
    "        \n",
    "        asset_types = {}\n",
    "        for asset in columns:\n",
    "            t = self.asset_map.get(asset, \"Unknown\")\n",
    "            asset_types.setdefault(t, []).append(asset)\n",
    "        \n",
    "        mats = []\n",
    "        lbs = []\n",
    "        ubs = []\n",
    "        names = []\n",
    "        for t, assets in asset_types.items():\n",
    "            vec = np.zeros(len(columns), dtype=float)\n",
    "            for a in assets:\n",
    "                idx = columns.get_loc(a)\n",
    "                vec[idx] = 1.0\n",
    "            mats.append(vec)\n",
    "            lbs.append(self.asset_lower.get(t, 0.0))\n",
    "            ubs.append(self.asset_upper.get(t, 1.0))\n",
    "            names.append(t)\n",
    "        \n",
    "        self.constraint_matrix = np.vstack(mats)        # shape = (n_types, n_assets)\n",
    "        self.lower_bounds = np.array(lbs, dtype=float)  # shape = (n_types,)\n",
    "        self.upper_bounds = np.array(ubs, dtype=float)  # shape = (n_types,)\n",
    "        self.asset_type_names = names\n",
    "        \n",
    "    # Final New QP with two phases - this allows for hard constraints on zero weights\n",
    "    def _apply_constraints_final(self, weights):\n",
    "        \"\"\"\n",
    "        ใช้ Quadratic Programming เพื่อบังคับ:\n",
    "        • Σw = 1\n",
    "        • per-asset cap 0.30 (SHV 0.40)\n",
    "        • asset_lower / asset_upper\n",
    "        • ลดการขยับจาก w0 โดยเฉพาะตำแหน่งที่ w0 == 0\n",
    "        \"\"\"\n",
    "        w0 = np.asarray(weights, float).copy()\n",
    "        n  = w0.size\n",
    "\n",
    "        # ---------- สร้างขอบรายตัว --------------------------------------------\n",
    "        ub = np.full(n, 0.30)\n",
    "        if self.asset_columns is not None and \"SHV\" in self.asset_columns:\n",
    "            ub[self.asset_columns.get_loc(\"SHV\")] = 0.40\n",
    "\n",
    "        lb = np.full(n, self.port_type[0])      # long_only → 0, long/short → -1\n",
    "        # (ถ้ามีพอร์ตชนิดอื่นปรับได้ตาม self.port_type)\n",
    "\n",
    "        # ---------- ตัวแปร QP ---------------------------------------------------\n",
    "        w = cp.Variable(n)\n",
    "\n",
    "        constraints = [\n",
    "            cp.sum(w) == 1,\n",
    "            w >= lb,\n",
    "            w <= ub\n",
    "        ]\n",
    "\n",
    "        # ---------- ข้อจำกัดรายหมวด -------------------------------------------\n",
    "        if hasattr(self, \"constraint_matrix\"):\n",
    "            C = self.constraint_matrix           # shape (n_types, n_assets)\n",
    "            constraints += [\n",
    "                C @ w >= self.lower_bounds,\n",
    "                C @ w <= self.upper_bounds\n",
    "            ]\n",
    "\n",
    "        # ---------- Objective: min Σ α_i (w_i - w0_i)^2 ------------------------\n",
    "        eps = 1e-4\n",
    "        alpha = 1.0 / (w0 + eps)        # ช่องที่ w0=0 จะถูกลงโทษมาก\n",
    "        #obj   = cp.Minimize(cp.sum(cp.multiply(alpha, cp.square(w - w0)))) # L2\n",
    "        obj = cp.Minimize(cp.sum(cp.multiply(alpha, cp.abs(w - w0)))) # L1\n",
    "\n",
    "        prob = cp.Problem(obj, constraints)\n",
    "\n",
    "        # เลือก solver ที่รองรับ QP\n",
    "        try:\n",
    "            prob.solve(solver=cp.OSQP)  # หรือ ECOS_BB / SCS\n",
    "        except cp.error.SolverError:\n",
    "            prob.solve(solver=cp.ECOS)\n",
    "\n",
    "        # ถ้าแก้ไม่ได้ (infeasible) กลับไปใช้วิธีเดิม\n",
    "        if w.value is None:\n",
    "            print(\"QP infeasible, falling back to greedy method.\")\n",
    "            return super()._apply_constraints(weights)\n",
    "\n",
    "        return np.asarray(w.value).flatten()\n",
    "    # ===== END V.2 =====\n",
    "    \n",
    "    # _apply_constraints_row_cvxpy\n",
    "    def _apply_constraints_row_cvxpy(self, weights_row, tol_zero=1e-6, lambda_zero=100.0):\n",
    "        \"\"\"\n",
    "        Applies two-phase QP constraints to a single row of weights.\n",
    "        - Phase 1: Attempts to solve with a hard lock on zero-weight assets.\n",
    "        - Phase 2: If Phase 1 is infeasible, it releases the lock and instead\n",
    "                   applies a heavy penalty for moving away from zero.\n",
    "        \"\"\"\n",
    "        # ---------- Data Preparation for a single row ----------\n",
    "        w0 = np.clip(weights_row, *self.port_type)\n",
    "        n = len(w0)\n",
    "        lower_w, _ = self.port_type\n",
    "\n",
    "        # Upper bound vector (0.30, except 0.40 for SHV)\n",
    "        ub_vec = np.full(n, 0.30)\n",
    "        if 'SHV' in self.asset_columns:\n",
    "            shv_idx = self.asset_columns.get_loc('SHV')\n",
    "            ub_vec[shv_idx] = 0.40\n",
    "\n",
    "        # Indices of assets with near-zero initial weights\n",
    "        zero_idx = np.where(w0 <= tol_zero)[0]\n",
    "\n",
    "        # ---------- Phase 1: Solve QP with locked zeros ----------\n",
    "        w_p1 = cp.Variable(n)\n",
    "        \n",
    "        # Base constraints + hard lock on zeros\n",
    "        cons_p1 = [w_p1 >= lower_w, w_p1 <= ub_vec]\n",
    "        if len(zero_idx) > 0:\n",
    "            cons_p1.append(w_p1[zero_idx] == 0)\n",
    "\n",
    "        # Add sum-to-one and group constraints\n",
    "        if self.port_type == long_only:\n",
    "            cons_p1.append(cp.sum(w_p1) == 1)\n",
    "        if hasattr(self, 'constraint_matrix'):\n",
    "            cm, lb, ub = self.constraint_matrix, self.lower_bounds, self.upper_bounds\n",
    "            cons_p1.extend([cm @ w_p1 >= lb, cm @ w_p1 <= ub])\n",
    "        \n",
    "        obj_p1 = cp.Minimize(cp.sum_squares(w_p1 - w0))\n",
    "        prob_p1 = cp.Problem(obj_p1, cons_p1)\n",
    "        prob_p1.solve(solver=cp.OSQP, warm_start=True)\n",
    "\n",
    "        if prob_p1.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "            return np.array(w_p1.value).flatten()\n",
    "\n",
    "        # ---------- Phase 2: Solve QP with penalty if Phase 1 failed ----------\n",
    "        print(\"⚠️ Strict (zero-locked) QP failed. Falling back to penalty-based QP.\")\n",
    "        w_p2 = cp.Variable(n)\n",
    "        \n",
    "        # Base constraints without the hard lock\n",
    "        cons_p2 = [w_p2 >= lower_w, w_p2 <= ub_vec]\n",
    "        if self.port_type == long_only:\n",
    "            cons_p2.append(cp.sum(w_p2) == 1)\n",
    "        if hasattr(self, 'constraint_matrix'):\n",
    "            cm, lb, ub = self.constraint_matrix, self.lower_bounds, self.upper_bounds\n",
    "            cons_p2.extend([cm @ w_p2 >= lb, cm @ w_p2 <= ub])\n",
    "            \n",
    "        # Softer objective with penalty for moving zero-weight assets\n",
    "        penalty = np.ones(n)\n",
    "        penalty[zero_idx] = lambda_zero\n",
    "        sqrt_p = np.sqrt(penalty)\n",
    "        obj_p2 = cp.Minimize(cp.sum_squares(cp.multiply(sqrt_p, w_p2 - w0)))\n",
    "        \n",
    "        prob_p2 = cp.Problem(obj_p2, cons_p2)\n",
    "        prob_p2.solve(solver=cp.OSQP, warm_start=True)\n",
    "\n",
    "        if prob_p2.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "            return np.array(w_p2.value).flatten()\n",
    "        else:\n",
    "            # Final fallback if both phases fail\n",
    "            print(f\"❌ Warning: Both QP attempts failed (Final status: {prob_p2.status}). Using fallback normalization.\")\n",
    "            s = w0.sum()\n",
    "            return (w0 / s) if s > 0 else np.ones_like(w0) / n\n",
    "    # ===== END =====\n",
    "\n",
    "    def build(self, input_shape, outputs, PE_type=pe_type):\n",
    "\n",
    "        d_model = 512\n",
    "        num_heads = 8\n",
    "        ff_dim = 2048\n",
    "        num_layers = 6\n",
    "        dropout_rate = 0.3\n",
    "        spe_kernel_size = 5\n",
    "\n",
    "        seq_len, feature_dim = input_shape\n",
    "\n",
    "        def get_positional_encoding(length, depth):\n",
    "            # Halve the depth because we will concatenate sine and cosine embeddings.\n",
    "            depth = depth / 2\n",
    "\n",
    "            # Create arrays for positions and depths.\n",
    "            positions = np.arange(length)[:, np.newaxis]      # Shape: (length, 1)\n",
    "            depths = np.arange(depth)[np.newaxis, :] / depth  # Shape: (1, depth)\n",
    "\n",
    "            # Calculate the angle rates.\n",
    "            angle_rates = 1 / (10000**depths)                 # Shape: (1, depth)\n",
    "\n",
    "            # Calculate the angle radians.\n",
    "            angle_rads = positions * angle_rates             # Shape: (length, depth)\n",
    "\n",
    "            # Concatenate the sine and cosine of the angle radians to form the encoding.\n",
    "            pos_encoding = np.concatenate(\n",
    "                [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "                axis=-1) \n",
    "            # Cast the final encoding to a float32 TensorFlow tensor.\n",
    "            return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "        \n",
    "        inputs = layers.Input(shape=(seq_len, feature_dim))\n",
    "\n",
    "        # ====== Position Encoding ======\n",
    "        if PE_type == 'OriPE':\n",
    "            x = layers.Dense(d_model)(inputs)\n",
    "            pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "            x = x + pos_encoding[tf.newaxis, :]\n",
    "        elif PE_type == 'Time2Vec':\n",
    "            time_embedding = Time2Vector(seq_len)(inputs)\n",
    "            x = layers.Concatenate(axis=-1)([inputs, time_embedding])\n",
    "            x = layers.Dense(d_model)(x)\n",
    "        elif PE_type == 'ConvSPE':\n",
    "            x = layers.Dense(d_model)(inputs)\n",
    "            x = ConvSPE(d_model=d_model, kernel_size=spe_kernel_size)(x)\n",
    "        elif PE_type == 'SineSPE':\n",
    "            x = layers.Dense(d_model)(inputs)\n",
    "            x = SineSPE(d_model=d_model, max_len=seq_len + 100)(x)\n",
    "        elif PE_type == 'TemporalPE':\n",
    "            x = layers.Dense(d_model)(inputs)\n",
    "            x = TemporalPositionalEncoding(d_model=d_model, max_len=seq_len + 100)(x)\n",
    "        elif PE_type == 'LearnablePE':\n",
    "            x = layers.Dense(d_model)(inputs)\n",
    "            pos_encoding_layer = LearnablePositionalEncoding(d_model=d_model, max_len=seq_len + 100, dropout=dropout_rate)\n",
    "            x = pos_encoding_layer(x)\n",
    "        elif PE_type == 'AbsolutePE':\n",
    "            x = layers.Dense(d_model)(inputs)\n",
    "            pos_encoding_layer = AbsolutePositionalEncoding(d_model=d_model, max_len=seq_len + 100, dropout=dropout_rate)\n",
    "            x = pos_encoding_layer(x)\n",
    "        elif PE_type == 'tAPE':\n",
    "            x = layers.Dense(d_model)(inputs)\n",
    "            pos_encoding_layer = tAPE(d_model=d_model, max_len=seq_len + 100, dropout=dropout_rate)\n",
    "            x = pos_encoding_layer(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown Positional Encoding type: {PE_type}\")\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "            attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "            out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "            ffn = layers.Dense(ff_dim, activation=\"relu\")(out1) # ori = relu\n",
    "            ffn = layers.Dense(d_model)(ffn)\n",
    "            ffn_output = layers.Dropout(dropout_rate)(ffn)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "        pooled = layers.GlobalAveragePooling1D()(x)       # (batch_size, d_model)\n",
    "        # outputs_layer = layers.Dense(outputs, activation=\"softmax\")(pooled)\n",
    "        \n",
    "        # use TemperatureSoftmax for softmax with temperature scaling\n",
    "        outputs_layer = layers.Dense(outputs)(pooled)\n",
    "        outputs_layer = TemperatureSoftmax(temperature=n_temp)(outputs_layer) \n",
    "          \n",
    "        # outputs_layer = layers.Dense(outputs, activation=TemperatureSoftmax(3.0))(pooled)  \n",
    "\n",
    "        model = KModel(inputs=inputs, outputs=outputs_layer)\n",
    "                     \n",
    "        # Modify Sharpe loss function\n",
    "        def sharpe_loss(_, weights):\n",
    "            w = weights[0]\n",
    "            # 1. คำนวณอนุกรมเวลาผลตอบแทนรายวันของพอร์ต\n",
    "            portfolio_returns_daily = tf.reduce_sum(tf.multiply(self.data, w), axis=1)\n",
    "            # 2. คำนวณค่าสถิติรายวัน\n",
    "            mean_daily_return = tf.reduce_mean(portfolio_returns_daily)\n",
    "            std_daily_return = tf.math.reduce_std(portfolio_returns_daily)\n",
    "            # 3. แปลงเป็นค่ารายปี (Annualize)\n",
    "            annualized_return = mean_daily_return * 252\n",
    "            annualized_volatility = std_daily_return * tf.sqrt(252.0) # ต้อง sqrt() จำนวนวัน\n",
    "            # 4. คำนวณ Sharpe Ratio ด้วยหน่วยเวลาที่สอดคล้องกัน\n",
    "            risk_free_rate = 0.02\n",
    "            sharpe = (annualized_return - risk_free_rate) / (annualized_volatility + 1e-8) # เพิ่ม epsilon เพื่อความเสถียร\n",
    "            return -sharpe\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        model.compile(loss=sharpe_loss, optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def calc_wgts(self, lkbk: int, ep: int, data: pd.DataFrame, features: pd.DataFrame, patience=10, PE_type=pe_type):\n",
    "        # Ensure constraint matrices exist\n",
    "        if not hasattr(self, \"constraint_matrix\"):\n",
    "            self._create_constraint_matrices(data.columns)\n",
    "        \n",
    "        print(f\"=== Calculating weights with lkbk={lkbk}, ep={ep}, patience={patience}, PE_type={PE_type} ===\")\n",
    "\n",
    "        # Scale the features\n",
    "        features = [features.shift(k).fillna(0).values[lkbk:] for k in range(lkbk)]\n",
    "        \n",
    "        # Create Numpy array from features\n",
    "        data_array = np.concatenate(features, axis=1)\n",
    "                    \n",
    "        # Split off train set\n",
    "        data = data.iloc[lkbk:]\n",
    "        \n",
    "        # Convert data to tensorflow format for processing in loss function\n",
    "        self.data = tf.cast(tf.constant(data), float)\n",
    "\n",
    "        # Building a new model (create fresh model for each rebalance)\n",
    "        self.model = self.build(data_array.shape, len(data.columns), PE_type=PE_type)\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "        # Adding a new axis to features\n",
    "        fit_predict_data = data_array[np.newaxis, :]\n",
    "\n",
    "        # Adding new axis to classifier\n",
    "        \"\"\"Extending the length of the output vector\"\"\"\n",
    "        y = np.zeros(len(data.columns))[np.newaxis, :]\n",
    "\n",
    "        print(f\"X shape: {fit_predict_data.shape}\")\n",
    "        # print(\"X:\", fit_predict_data)\n",
    "        print(\"y shape:\", ep)\n",
    "        # print(\"y:\", y)\n",
    "        # Fit the model  \n",
    "        self.model.fit(fit_predict_data, y, epochs=ep, shuffle=False, callbacks=[early_stopping])\n",
    "\n",
    "        # Predict weights\n",
    "        raw_weights = self.model.predict(fit_predict_data)[0]\n",
    "        raw_weights_non_zero_count = np.count_nonzero(raw_weights)\n",
    "        print(f\"Raw Weights selected ETF: {raw_weights_non_zero_count}\")\n",
    "        # Apply constraints\n",
    "        weights = self._apply_constraints_final(raw_weights)\n",
    "        return weights, raw_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ff619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asset_weights = model.calc_wgts(lookback, n_epochs, train_features, train_features, patience=10)\n",
    "def quarterly_walk_forward(df, lookback, n_epochs, features, asset_map=None, \n",
    "                                   asset_lower=None, asset_upper=None, port_type=(0, 1), \n",
    "                                   start_year=2020, trading_days_per_quarter=63, \n",
    "                                   min_train_periods=252, PE_type=pe_type, n_patience=10):\n",
    "    \"\"\"Optimized quarterly walk-forward analysis\"\"\"\n",
    "    \n",
    "    # Efficient data preparation\n",
    "    original_index = df.index\n",
    "    df_reset = df.reset_index(drop=True)\n",
    "    features_reset = features.reset_index(drop=True)\n",
    "    \n",
    "    # Get rebalance dates\n",
    "    rebalance_dates = get_rebalance_dates(df, start_year)\n",
    "    \n",
    "    # Pre-allocate result containers\n",
    "    all_rets = []\n",
    "    weights_list = []\n",
    "    raw_weights_list = []\n",
    "    rebalance_info = []\n",
    "    \n",
    "    # Initialize model once\n",
    "    model = Model(\n",
    "        max_weight=1, \n",
    "        asset_map=asset_map, \n",
    "        asset_lower=asset_lower,\n",
    "        asset_upper=asset_upper, \n",
    "        port_type=port_type\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Starting Optimized Quarterly Rebalancing Analysis ===\")\n",
    "    print(f\"Training lookback period: {lookback} days\")\n",
    "    print(f\"Minimum training periods: {min_train_periods} days\")\n",
    "    print(f\"Trading days per quarter: {trading_days_per_quarter} days\")\n",
    "    print(f\"Total rebalance periods: {len(rebalance_dates)}\")\n",
    "    \n",
    "    for i, rebalance_date in enumerate(rebalance_dates):\n",
    "        print(f\"\\n--- Rebalancing {i+1}/{len(rebalance_dates)} ---\")\n",
    "        print(f\"Rebalance Date: {rebalance_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        try:\n",
    "            # More efficient position finding\n",
    "            train_end_pos = original_index.get_indexer([rebalance_date], method='pad')[0] - 1\n",
    "            if train_end_pos < 0:\n",
    "                continue\n",
    "                \n",
    "        except Exception:\n",
    "            print(f\"Cannot find position for {rebalance_date}, skipping... ❌❌❌\")\n",
    "            continue\n",
    "        \n",
    "        # Define training period\n",
    "        train_start_pos = max(0, train_end_pos - min_train_periods)\n",
    "        \n",
    "        print(f\"Training period: {original_index[train_start_pos].strftime('%Y-%m-%d')} to {original_index[train_end_pos].strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Training days: {train_end_pos - train_start_pos + 1}\")\n",
    "        \n",
    "        # Get quarter end\n",
    "        quarter_end = get_quarter_end_date(rebalance_date, original_index, trading_days_per_quarter)\n",
    "        \n",
    "        try:\n",
    "            test_end_pos = original_index.get_loc(quarter_end)\n",
    "            print(f\"Test period: {rebalance_date.strftime('%Y-%m-%d')} to {quarter_end.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"Test days: {test_end_pos - train_end_pos}\")\n",
    "        except Exception:\n",
    "            print(f\"Cannot find end date for quarter, skipping... ❌❌❌\")\n",
    "            continue\n",
    "        \n",
    "        # Skip if insufficient future data\n",
    "        if test_end_pos >= len(df_reset):\n",
    "            print(f\"Not enough future data, stopping at rebalance {i+1} ❌❌❌\")\n",
    "            break\n",
    "        \n",
    "        # Extract and prepare training data more efficiently\n",
    "        train_data = df_reset.iloc[train_start_pos:train_end_pos+1].copy()\n",
    "        train_features = features_reset.iloc[train_start_pos:train_end_pos+1].copy()\n",
    "        \n",
    "        # Efficient data cleaning\n",
    "        train_data = train_data.ffill().bfill()\n",
    "        train_features = train_features.fillna(0) # for cal sharpe loss (ori pct_change)\n",
    "        # train_features_roll = train_features.rolling(window=5, min_periods=1).mean().fillna(0)\n",
    "\n",
    "        # # # scale features - for training\n",
    "        #sc = StandardScaler()\n",
    "        #train_features_sc = train_features.copy()\n",
    "        # train_features_sc = train_features_sc.rolling(window=5, min_periods=1).mean().fillna(0)\n",
    "        #train_features_sc = pd.DataFrame(sc.fit_transform(train_features_sc), columns=train_features.columns, index=train_features.index)\n",
    "        \n",
    "        # Calculate test returns more efficiently\n",
    "        test_data = df_reset.iloc[train_end_pos:test_end_pos+1].copy()\n",
    "        test_returns = test_data.pct_change().fillna(0).iloc[1:]\n",
    "\n",
    "        print(f\"Training data shape: {train_data.shape}\")\n",
    "        print(f\"Test returns shape: {test_returns.shape}\")\n",
    "        \n",
    "        # Train model and get weights\n",
    "        try:\n",
    "            # Clear session for memory management\n",
    "            K.clear_session()\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            # asset_weights = model.calc_wgts(lookback, n_epochs, train_data, train_features, patience=10)\n",
    "            asset_weights, raw_weights = model.calc_wgts(lookback, n_epochs, \n",
    "                                                         train_features, train_features, \n",
    "                                                         patience=n_patience, \n",
    "                                                         PE_type=PE_type)\n",
    "            #asset_weights, raw_weights = model.calc_wgts(lookback, n_epochs, train_features, train_features_sc, patience=10)\n",
    "\n",
    "            print(f\"\\n{'=' * 30}\")\n",
    "            # total weight should be 1 status\n",
    "            total_weight = np.sum(asset_weights).round(4)\n",
    "            if total_weight == 1:\n",
    "                print(f\"Total weight: {total_weight:.4f} ✅\")\n",
    "            else:\n",
    "                print(f\"Total weight: {total_weight:.4f} ❌ (should be 1.0)\")\n",
    "            \n",
    "            weights_non_zero_count = np.count_nonzero(asset_weights)\n",
    "            print(f\"Selected ETF count: {weights_non_zero_count}\")\n",
    "            # if weights_non_zero_count < 10 or weights_non_zero_count > 15:\n",
    "            #     print(f\"Warning: weights have {weights_non_zero_count} ❌\")\n",
    "            #     os.system(f'say \"Warning: Selected ETF have {weights_non_zero_count}\"')\n",
    "            # if weights_non_zero_count >= 10 and weights_non_zero_count <= 15:\n",
    "            #     print(f\"Selected ETF: {weights_non_zero_count} ✅\")\n",
    "\n",
    "            # Print allocation summary\n",
    "            if hasattr(model, 'constraint_matrix') and hasattr(model, 'asset_type_names'):\n",
    "                type_weights = np.dot(model.constraint_matrix, asset_weights)\n",
    "                print(\"Asset Type Allocation:\")\n",
    "                for j, (asset_type, weight) in enumerate(zip(model.asset_type_names, type_weights)):\n",
    "                    lower = model.asset_lower.get(asset_type, 0.0)\n",
    "                    upper = model.asset_upper.get(asset_type, 1.0)\n",
    "                    # status\n",
    "                    if lower <= weight.round(4) <= upper:\n",
    "                        status = \"✅\" #Within Range\n",
    "                    else:\n",
    "                        status = \"❌\" #Out of Range\n",
    "                    print(f\"  {asset_type}: {weight:.3f} ({weight*100:.2f}%) [Range: {lower:.2f}-{upper:.2f}] {status}\")\n",
    "            \n",
    "            # Calculate out-of-sample returns\n",
    "            if len(test_returns) > 0:\n",
    "                # Vectorized return calculation\n",
    "                oos_returns = (test_returns.values * asset_weights).sum(axis=1)\n",
    "                oos_returns = pd.Series(oos_returns, index=test_returns.index)\n",
    "                \n",
    "                all_rets.append(oos_returns)\n",
    "                weights_list.append(asset_weights)\n",
    "                raw_weights_list.append(raw_weights)\n",
    "                rebalance_info.append({\n",
    "                    'rebalance_date': rebalance_date,\n",
    "                    'quarter_end': quarter_end,\n",
    "                    'quarter_return': oos_returns.sum(),\n",
    "                    'quarter_days': len(oos_returns)\n",
    "                })\n",
    "                \n",
    "                print(f\"Quarter return: {oos_returns.sum():.4f} ({oos_returns.sum()*100:.2f}%)\")\n",
    "            else:\n",
    "                print(\"No test returns available ❌❌❌\")\n",
    "            \n",
    "            print(f\"\\n{'=' * 30}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in rebalancing: {str(e)} ❌❌❌\")\n",
    "            continue\n",
    "    \n",
    "    # Combine results efficiently\n",
    "    if all_rets:\n",
    "        pnl = pd.concat(all_rets, ignore_index=False)\n",
    "        weights_df = pd.DataFrame(weights_list, columns=df.columns)\n",
    "        raw_weights_df = pd.DataFrame(raw_weights_list, columns=df.columns)\n",
    "        rebalance_summary = pd.DataFrame(rebalance_info)\n",
    "        \n",
    "        print(f\"\\n=== Rebalancing Summary ===\")\n",
    "        print(f\"Total quarters processed: {len(rebalance_info)}\")\n",
    "        print(f\"Total return periods: {len(pnl)}\")\n",
    "        print(f\"Average quarterly return: {rebalance_summary['quarter_return'].mean():.4f}\")\n",
    "        print(f\"Quarterly return std: {rebalance_summary['quarter_return'].std():.4f}\")\n",
    "        \n",
    "        return pnl, weights_df, rebalance_summary, raw_weights_df\n",
    "    else:\n",
    "        print(\"No successful rebalancing periods ❌❌❌\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data.copy()\n",
    "features = data_train.pct_change().fillna(0)  # Calculate percentage change for features\n",
    "\n",
    "# Reset states generated by Keras\n",
    "K.clear_session()\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "n_lookback = avg_days * 12 # Sequence: โค้ดจะสร้าง Input โดยสำหรับ ทุกๆ วัน ในชุดข้อมูลเทรน มันจะ \"มองย้อนกลับไป\" เป็นจำนวน n_lookback วัน (เช่น 21 วันทำการ หรือประมาณ 1 เดือน)\n",
    "n_trading_days_per_quarter = avg_days * 3  # 21 days/month * 3 months = 63 days\n",
    "n_min_train_periods = avg_days * 12 * 3  # 3 years of training data = 252*3 days = 756 days\n",
    "\n",
    "print(f\"Lookback period: {n_lookback} days\")\n",
    "print(f\"Trading days per quarter: {n_trading_days_per_quarter} days\")\n",
    "print(f\"Minimum training periods: {n_min_train_periods} days\")\n",
    "\n",
    "# Run quarterly rebalancing with constraints\n",
    "pnl, model_weights, rebalance_summary, Model_raw_weights = quarterly_walk_forward(\n",
    "    data_train, \n",
    "    lookback=n_lookback, \n",
    "    n_epochs=50, \n",
    "    features=features.fillna(0),\n",
    "    asset_map=asset_map, \n",
    "    asset_lower=asset_lower_aggressive,\n",
    "    asset_upper=asset_upper_aggressive,\n",
    "    port_type=long_only,\n",
    "    start_year=start_rebalance_year,\n",
    "    trading_days_per_quarter=n_trading_days_per_quarter,  \n",
    "    min_train_periods=n_min_train_periods,\n",
    "    PE_type=pe_type,\n",
    "    n_patience=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c2f8b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_dates =  get_rebalance_dates(data, start_year=start_rebalance_year)\n",
    "\n",
    "# add rebalance_dates to weights_df index\n",
    "weights_df = model_weights.copy()\n",
    "weights_df.index = rebalance_dates[:len(weights_df)]\n",
    "print(\"\\n%Weights DataFrame with Rebalance Dates:\")\n",
    "# display(weights_df.multiply(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7672ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weights_df = Model_raw_weights.copy()\n",
    "raw_weights_df.index = rebalance_dates[:len(raw_weights_df)]\n",
    "\n",
    "\n",
    "# print(\"\\n%Raw Weights DataFrame with Rebalance Dates:\")\n",
    "# display(raw_weights_df.multiply(100).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the Check and Print Results ---\n",
    "violations_found = check_portfolio_constraints(\n",
    "    weights_df, \n",
    "    asset_map, \n",
    "    asset_lower_aggressive, \n",
    "    asset_upper_aggressive\n",
    ")\n",
    "\n",
    "if not violations_found:\n",
    "    print(\"✅ All portfolio weights satisfy the constraints.\")\n",
    "else:\n",
    "    print(\"❌ Constraint violations were found:\")\n",
    "    for date, messages in violations_found.items():\n",
    "        print(f\"\\nOn {date}:\")\n",
    "        for msg in messages:\n",
    "            print(f\"  - {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d678ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_df.to_csv(f'{output_dir}/{train_type}/raw_weights_df_run{run_no}.csv', index=True)\n",
    "\n",
    "save_dataframe_to_new_sheet(raw_weights_df, results_excel_path, 'Weights_Before_PostNorm')\n",
    "save_dataframe_to_new_sheet(weights_df, results_excel_path, 'Weights_After_PostNorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3072e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('say \"Model training has finished\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fe4b4",
   "metadata": {},
   "source": [
    "# Compared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d1de2",
   "metadata": {},
   "source": [
    "### 1. Model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86527557",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_df = weights_df.copy()\n",
    "model_weights_df = model_weights_df.round(4)\n",
    "model_weights_df = model_weights_df.abs()\n",
    "model_weights_df\n",
    "save_dataframe_to_new_sheet(model_weights_df, results_excel_path, 'Model Weights')\n",
    "# model_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89422987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of weights > 0 in each row\n",
    "non_zero_weights_count = (model_weights_df > 0).sum(axis=1)\n",
    "print(\"\\nNumber of non-zero weights in each row:\")\n",
    "print(non_zero_weights_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e1f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total weight in each row to ensure it sums to 1\n",
    "total_weights = model_weights_df.sum(axis=1).round(2)\n",
    "print(\"\\nTotal weights in each row (should be 1):\")\n",
    "print(total_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7c7e1",
   "metadata": {},
   "source": [
    "### 2. Traditional mean-variance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ea45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free = 0.02  # Example risk-free rate\n",
    "mvo_weights_df = mvo_quarterly_rebalancing(data, asset_map, \n",
    "                                           asset_lower_aggressive, asset_upper_aggressive, \n",
    "                                           port_type, start_year=start_rebalance_year,\n",
    "                                           trading_days_per_quarter=n_trading_days_per_quarter,\n",
    "                                           min_train_periods=n_min_train_periods,\n",
    "                                           risk_free_rate=risk_free)\n",
    "\n",
    "mvo_weights_df = mvo_weights_df.round(4)\n",
    "save_dataframe_to_new_sheet(mvo_weights_df, results_excel_path, 'MVO Weights')\n",
    "# mvo_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1996f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Run the Check and Print Results ---\n",
    "# violations_found = check_portfolio_constraints(\n",
    "#     mvo_weights_df, \n",
    "#     asset_map, \n",
    "#     asset_lower_aggressive, \n",
    "#     asset_upper_aggressive\n",
    "# )\n",
    "\n",
    "# if not violations_found:\n",
    "#     print(\"✅ All portfolio weights satisfy the constraints.\")\n",
    "# else:\n",
    "#     print(\"❌ Constraint violations were found:\")\n",
    "#     for date, messages in violations_found.items():\n",
    "#         print(f\"\\nOn {date}:\")\n",
    "#         for msg in messages:\n",
    "#             print(f\"  - {msg}\")\n",
    "\n",
    "# # check number of weights > 0 in each row\n",
    "# non_zero_weights_count = (mvo_weights_df > 0).sum(axis=1)\n",
    "# print(\"\\nNumber of non-zero weights in each row:\")\n",
    "# print(non_zero_weights_count)\n",
    "\n",
    "# # check total weight in each row to ensure it sums to 1\n",
    "# total_weights = mvo_weights_df.sum(axis=1).round(2)\n",
    "# print(\"\\nTotal weights in each row (should be 1):\")\n",
    "# print(total_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a622345",
   "metadata": {},
   "source": [
    "### 3. Equal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1047e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run equal weight portfolio\n",
    "equal_weights_df = equal_weight_portfolio(data, rebalance_dates)\n",
    "equal_weights_df = equal_weights_df.round(4)\n",
    "save_dataframe_to_new_sheet(equal_weights_df, results_excel_path, 'Equal Weights')\n",
    "# equal_weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263dcf5d",
   "metadata": {},
   "source": [
    "### 4. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ed02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_df = pd.DataFrame(\n",
    "   index=rebalance_dates,\n",
    "   columns=data.columns,\n",
    "   data=0.0\n",
    ")\n",
    "num_cash = sum(1 for v in asset_map.values() if v == 'Cash_Equivalent')\n",
    "num_fixed_income = sum(1 for v in asset_map.values() if v == 'Fixed_Income')\n",
    "num_alternatives = sum(1 for v in asset_map.values() if v == 'Alternatives')\n",
    "print(f\"Number of Cash Equivalents: {num_cash}\")\n",
    "print(f\"Number of Fixed Income: {num_fixed_income}\")\n",
    "print(f\"Number of Alternatives: {num_alternatives}\")\n",
    "\n",
    "benchmark_df['SHV'] = 0.05 / num_cash\n",
    "benchmark_df[['BND', 'BNDX', 'JNK']] = 0.05 / num_fixed_income\n",
    "benchmark_df['VT'] = 0.75\n",
    "benchmark_df[['REET', 'IGF', 'PDBC', 'GLD']] = 0.15 / num_alternatives\n",
    "save_dataframe_to_new_sheet(benchmark_df, results_excel_path, 'Beanchmark Weights')\n",
    "# benchmark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb944a61",
   "metadata": {},
   "source": [
    "### 4. Compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b19de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main analysis\n",
    "start_date = rebalance_dates[0]  # First rebalance date\n",
    "\n",
    "# Calculate portfolio returns for each strategy\n",
    "portfolios = {\n",
    "    'Model Portfolio': model_weights_df,\n",
    "    'MVO Portfolio': mvo_weights_df,\n",
    "    'Equal Weight': equal_weights_df,\n",
    "    'Benchmark': benchmark_df\n",
    "}\n",
    "\n",
    "portfolio_returns = {}\n",
    "for name, weights in portfolios.items():\n",
    "    returns = calculate_portfolio_returns(data, weights, rebalance_dates, start_date)\n",
    "    portfolio_returns[name] = returns\n",
    "\n",
    "# Calculate performance metrics\n",
    "performance_metrics = {}\n",
    "benchmark_returns = portfolio_returns['Benchmark']\n",
    "\n",
    "for name, returns in portfolio_returns.items():\n",
    "    if name == 'Benchmark':\n",
    "        metrics = calculate_performance_metrics(returns, returns)  # Self as benchmark\n",
    "    else:\n",
    "        metrics = calculate_performance_metrics(returns, benchmark_returns)\n",
    "    performance_metrics[name] = metrics\n",
    "\n",
    "# Create performance comparison DataFrame\n",
    "performance_df = pd.DataFrame(performance_metrics).T\n",
    "print(\"Portfolio Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "# performance_df.to_csv(f'{output_dir}/{train_type}/performance_comparison_run{run_no}.csv')\n",
    "save_dataframe_to_new_sheet(performance_df.T, results_excel_path, 'Performance Comparison')\n",
    "performance_df.round(4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b8f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75167f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=('Cumulative Returns', 'Maximum Drawdown'),\n",
    "    vertical_spacing=0.12,\n",
    "    row_heights=[0.7, 0.3]\n",
    ")\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Plot cumulative returns\n",
    "for i, (name, returns) in enumerate(portfolio_returns.items()):\n",
    "    #if name != 'Benchmark':\n",
    "    cumulative_returns = (1 + returns).cumprod()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=cumulative_returns.index,\n",
    "            y=cumulative_returns.values,\n",
    "            mode='lines',\n",
    "            name=name,\n",
    "            line=dict(color=colors[i], width=2),\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Plot drawdowns\n",
    "for i, (name, returns) in enumerate(portfolio_returns.items()):\n",
    "#if name != 'Benchmark':\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    rolling_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - rolling_max) / rolling_max * 100\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=drawdown.index,\n",
    "            y=drawdown.values,\n",
    "            mode='lines',\n",
    "            name=name,\n",
    "            line=dict(color=colors[i], width=2),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Portfolio Performance Comparison',\n",
    "    height=800,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Cumulative Return\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Drawdown (%)\", row=2, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the plot to a file\n",
    "fig.write_html(f'{output_dir}/{train_type}/portfolio_performance_{model_type}_run_{run_no}.html')\n",
    "# save image to file\n",
    "fig.write_image(f'{output_dir}/{train_type}/portfolio_performance_{model_type}_run_{run_no}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nKey Performance Highlights:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "best_return = performance_df['Annualized Return (%)'].idxmax()\n",
    "print(f\"Best Annualized Return: {best_return} ({performance_df.loc[best_return, 'Annualized Return (%)']:.2f}%)\")\n",
    "\n",
    "best_sharpe = performance_df['Sharpe Ratio'].idxmax()\n",
    "print(f\"Best Sharpe Ratio: {best_sharpe} ({performance_df.loc[best_sharpe, 'Sharpe Ratio']:.3f})\")\n",
    "\n",
    "best_sortino = performance_df['Sortino Ratio'].idxmax()\n",
    "print(f\"Best Sortino Ratio: {best_sortino} ({performance_df.loc[best_sortino, 'Sortino Ratio']:.3f})\")\n",
    "\n",
    "lowest_dd = performance_df['Max Drawdown (%)'].idxmax()  # Most negative (lowest)\n",
    "print(f\"Lowest Max Drawdown: {lowest_dd} ({performance_df.loc[lowest_dd, 'Max Drawdown (%)']:.2f}%)\")\n",
    "\n",
    "lowest_vol = performance_df['Volatility (%)'].idxmin()\n",
    "print(f\"Lowest Volatility: {lowest_vol} ({performance_df.loc[lowest_vol, 'Volatility (%)']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('say \"Model comparison has finished\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34cafe",
   "metadata": {},
   "source": [
    "# Quarterly Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quarterly periods\n",
    "quarterly_periods = get_quarterly_periods(rebalance_dates, data)\n",
    "quarterly_periods\n",
    "\n",
    "# Portfolio definitions\n",
    "portfolios = {\n",
    "    'Model Portfolio': model_weights_df,\n",
    "    'MVO Portfolio': mvo_weights_df,\n",
    "    'Equal Weight': equal_weights_df,\n",
    "    'Benchmark': benchmark_df\n",
    "}\n",
    "\n",
    "# Calculate quarterly performance for each portfolio\n",
    "quarterly_results = {}\n",
    "weights_df_2 = weights_df.copy\n",
    "for portfolio_name, weights_df_2 in portfolios.items():\n",
    "    quarterly_results[portfolio_name] = {}\n",
    "    \n",
    "    for period in quarterly_periods:\n",
    "        quarter = period['quarter']\n",
    "        returns = calculate_quarterly_portfolio_returns(data, weights_df_2, period)\n",
    "        metrics = calculate_quarterly_metrics(returns)\n",
    "        quarterly_results[portfolio_name][quarter] = metrics\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "all_metrics = ['Total Return (%)', 'Annualized Return (%)',\n",
    "               'Volatility (%)', 'Max Drawdown (%)', \n",
    "               'Max Drawdown Duration (days)', 'Sharpe Ratio', 'Sortino Ratio']\n",
    "quarterly_comparison = {}\n",
    "\n",
    "for metric in all_metrics:\n",
    "    quarterly_comparison[metric] = pd.DataFrame({\n",
    "        portfolio: {quarter: quarterly_results[portfolio][quarter][metric] \n",
    "                   for quarter in quarterly_results[portfolio]}\n",
    "        for portfolio in portfolios.keys()\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for metric in all_metrics:\n",
    "    print(f\"\\n{metric}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(quarterly_comparison[metric].round(4))\n",
    "    # quarterly_comparison[metric].to_csv(f'{output_dir}/{train_type}/quarterly_{metric.lower().replace(\" \", \"_\")}_run{run_no}.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e561ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- เริ่มโค้ดสำหรับบันทึก Excel ---\n",
    "qoq_excel_path = f\"{output_dir}/{train_type}/02_Quarterly_{model_type}_{pe_type}_{pre_post}_run_{run_no}.xlsx\"\n",
    "\n",
    "for metric in all_metrics:\n",
    "    sheet_name = metric.replace(\" \", \"_\")\n",
    "    df_to_save = quarterly_comparison[metric].round(4)\n",
    "    save_dataframe_to_new_sheet(df_to_save, qoq_excel_path, sheet_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Quarterly Returns (%)', 'Quarterly Volatility (%)', \n",
    "                   'Quarterly Sharpe Ratio', 'Quarterly Max Drawdown (%)'),\n",
    "    vertical_spacing=0.2,\n",
    "    horizontal_spacing=0.2\n",
    ")\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "quarters = list(quarterly_comparison['Total Return (%)'].index)\n",
    "\n",
    "# Plot quarterly returns\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=quarters,\n",
    "            y=quarterly_comparison['Total Return (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=quarters,\n",
    "            y=quarterly_comparison['Volatility (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=quarters,\n",
    "            y=quarterly_comparison['Sharpe Ratio'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=quarters,\n",
    "            y=quarterly_comparison['Max Drawdown (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Quarterly Portfolio Performance Comparison',\n",
    "    height=700,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig.update_yaxes(title_text=\"Return (%)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Volatility (%)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Sharpe Ratio\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Max Drawdown (%)\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Summary Statistics by Quarter\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUARTERLY PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for quarter in quarters:\n",
    "    print(f\"\\n{quarter}:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Best performing portfolio this quarter\n",
    "    quarter_returns = quarterly_comparison['Total Return (%)'].loc[quarter]\n",
    "    best_return = quarter_returns.idxmax()\n",
    "    print(f\"Best Return: {best_return} ({quarter_returns[best_return]:.2f}%)\")\n",
    "    \n",
    "    # Best Sharpe ratio this quarter\n",
    "    quarter_sharpe = quarterly_comparison['Sharpe Ratio'].loc[quarter].dropna()\n",
    "    if not quarter_sharpe.empty:\n",
    "        best_sharpe = quarter_sharpe.idxmax()\n",
    "        print(f\"Best Sharpe: {best_sharpe} ({quarter_sharpe[best_sharpe]:.3f})\")\n",
    "    \n",
    "    # Lowest volatility this quarter\n",
    "    quarter_vol = quarterly_comparison['Volatility (%)'].loc[quarter].dropna()\n",
    "    if not quarter_vol.empty:\n",
    "        lowest_vol = quarter_vol.idxmin()\n",
    "        print(f\"Lowest Vol: {lowest_vol} ({quarter_vol[lowest_vol]:.2f}%)\")\n",
    "\n",
    "# Ranking Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PORTFOLIO RANKINGS BY QUARTER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ranking_df = pd.DataFrame(index=quarters, columns=portfolios.keys())\n",
    "\n",
    "for quarter in quarters:\n",
    "    # Rank by quarterly returns (1 = best)\n",
    "    quarter_returns = quarterly_comparison['Total Return (%)'].loc[quarter]\n",
    "    ranks = quarter_returns.rank(ascending=False, method='min')\n",
    "    ranking_df.loc[quarter] = ranks\n",
    "\n",
    "print(\"\\nRanking by Quarterly Returns (1=Best, 4=Worst):\")\n",
    "print(ranking_df.astype(int))\n",
    "\n",
    "# Average ranking\n",
    "avg_ranking = ranking_df.mean().sort_values()\n",
    "print(f\"\\nAverage Ranking Across All Quarters:\")\n",
    "print(\"-\" * 40)\n",
    "for portfolio, avg_rank in avg_ranking.items():\n",
    "    print(f\"{portfolio}: {avg_rank:.2f}\")\n",
    "\n",
    "# Win rate analysis\n",
    "print(f\"\\nQuarterly Win Rate (% of quarters ranked #1):\")\n",
    "print(\"-\" * 45)\n",
    "for portfolio in portfolios.keys():\n",
    "    win_rate = (ranking_df[portfolio] == 1).sum() / len(quarters) * 100\n",
    "    print(f\"{portfolio}: {win_rate:.1f}%\")\n",
    "\n",
    "# Save the plot to a file\n",
    "fig.write_html(f'{output_dir}/{train_type}/QoQ_performance_{model_type}_run_{run_no}.html')\n",
    "fig.write_image(f'{output_dir}/{train_type}/QoQ_performance_{model_type}_run_{run_no}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf5028f",
   "metadata": {},
   "source": [
    "### Yearly Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b51084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Analysis\n",
    "print(\"Yearly Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get yearly periods\n",
    "yearly_periods = get_yearly_periods(rebalance_dates, data)\n",
    "\n",
    "# Portfolio definitions\n",
    "portfolios = {\n",
    "    'Model Portfolio': model_weights_df,\n",
    "    'MVO Portfolio': mvo_weights_df,\n",
    "    'Equal Weight': equal_weights_df,\n",
    "    'Benchmark': benchmark_df\n",
    "}\n",
    "\n",
    "# Calculate yearly performance for each portfolio\n",
    "yearly_results = {}\n",
    "weights_df_2 = weights_df.copy()\n",
    "for portfolio_name, weights_df_2 in portfolios.items():\n",
    "    yearly_results[portfolio_name] = {}\n",
    "    \n",
    "    for period in yearly_periods:\n",
    "        year = period['year']\n",
    "        returns = calculate_yearly_portfolio_returns(data, weights_df_2, period, rebalance_dates)\n",
    "        metrics = calculate_yearly_metrics(returns)\n",
    "        yearly_results[portfolio_name][year] = metrics\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "all_metrics = ['Total Return (%)', 'Annualized Return (%)', 'Volatility (%)', \n",
    "               'Sharpe Ratio', 'Sortino Ratio', 'Max Drawdown (%)', \n",
    "               'Max DD Duration (days)', 'VaR 95% (%)', 'Calmar Ratio', 'Trading Days']\n",
    "\n",
    "yearly_comparison = {}\n",
    "\n",
    "for metric in all_metrics:\n",
    "    yearly_comparison[metric] = pd.DataFrame({\n",
    "        portfolio: {year: yearly_results[portfolio][year][metric] \n",
    "                   for year in yearly_results[portfolio]}\n",
    "        for portfolio in portfolios.keys()\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for metric in all_metrics:\n",
    "    print(f\"\\n{metric}\")\n",
    "    print(\"-\" * 40)\n",
    "    if metric in ['Trading Days']:\n",
    "        print(yearly_comparison[metric].astype(int))\n",
    "    else:\n",
    "        print(yearly_comparison[metric].round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Annual Returns (%)', 'Annual Volatility (%)', \n",
    "                   'Annual Sharpe Ratio', 'Annual Max Drawdown (%)'),\n",
    "                   #'Annual Sortino Ratio', 'Annual Calmar Ratio'),\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "years = list(yearly_comparison['Total Return (%)'].index)\n",
    "\n",
    "# Plot annual returns\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=years,\n",
    "            y=yearly_comparison['Total Return (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Plot annual volatility\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=years,\n",
    "            y=yearly_comparison['Volatility (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Plot annual Sharpe ratio\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=years,\n",
    "            y=yearly_comparison['Sharpe Ratio'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Plot annual max drawdown\n",
    "for i, portfolio in enumerate(portfolios.keys()):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=years,\n",
    "            y=yearly_comparison['Max Drawdown (%)'][portfolio],\n",
    "            name=portfolio,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# # Plot annual Sortino ratio\n",
    "# for i, portfolio in enumerate(portfolios.keys()):\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=years,\n",
    "#             y=yearly_comparison['Sortino Ratio'][portfolio],\n",
    "#             mode='lines+markers',\n",
    "#             name=portfolio,\n",
    "#             line=dict(color=colors[i]),\n",
    "#             showlegend=False\n",
    "#         ),\n",
    "#         row=3, col=1\n",
    "#     )\n",
    "\n",
    "# # Plot annual Calmar ratio\n",
    "# for i, portfolio in enumerate(portfolios.keys()):\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=years,\n",
    "#             y=yearly_comparison['Calmar Ratio'][portfolio],\n",
    "#             mode='lines+markers',\n",
    "#             name=portfolio,\n",
    "#             line=dict(color=colors[i]),\n",
    "#             showlegend=False\n",
    "#         ),\n",
    "#         row=3, col=2\n",
    "#     )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Annual Portfolio Performance Comparison',\n",
    "    height=900,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig.update_yaxes(title_text=\"Return (%)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Volatility (%)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Sharpe Ratio\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Max Drawdown (%)\", row=2, col=2)\n",
    "# fig.update_yaxes(title_text=\"Sortino Ratio\", row=3, col=1)\n",
    "# fig.update_yaxes(title_text=\"Calmar Ratio\", row=3, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the plot to a file\n",
    "fig.write_html(f'{output_dir}/{train_type}/YoY_performance_{model_type}_run_{run_no}.html')\n",
    "fig.write_image(f'{output_dir}/{train_type}/YoY_performance_{model_type}_run_{run_no}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b74cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics by Year\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANNUAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for year in years:\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Best performing portfolio this year\n",
    "    year_returns = yearly_comparison['Total Return (%)'].loc[year]\n",
    "    best_return = year_returns.idxmax()\n",
    "    print(f\"Best Return: {best_return} ({year_returns[best_return]:.2f}%)\")\n",
    "    \n",
    "    # Best Sharpe ratio this year\n",
    "    year_sharpe = yearly_comparison['Sharpe Ratio'].loc[year].dropna()\n",
    "    if not year_sharpe.empty:\n",
    "        best_sharpe = year_sharpe.idxmax()\n",
    "        print(f\"Best Sharpe: {best_sharpe} ({year_sharpe[best_sharpe]:.3f})\")\n",
    "    \n",
    "    # Lowest volatility this year\n",
    "    year_vol = yearly_comparison['Volatility (%)'].loc[year].dropna()\n",
    "    if not year_vol.empty:\n",
    "        lowest_vol = year_vol.idxmin()\n",
    "        print(f\"Lowest Vol: {lowest_vol} ({year_vol[lowest_vol]:.2f}%)\")\n",
    "    \n",
    "    # Best Calmar ratio this year\n",
    "    year_calmar = yearly_comparison['Calmar Ratio'].loc[year].dropna()\n",
    "    if not year_calmar.empty:\n",
    "        best_calmar = year_calmar.idxmax()\n",
    "        print(f\"Best Calmar: {best_calmar} ({year_calmar[best_calmar]:.3f})\")\n",
    "\n",
    "# Ranking Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PORTFOLIO RANKINGS BY YEAR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ranking_df = pd.DataFrame(index=years, columns=portfolios.keys())\n",
    "\n",
    "for year in years:\n",
    "    # Rank by annual returns (1 = best)\n",
    "    year_returns = yearly_comparison['Total Return (%)'].loc[year]\n",
    "    ranks = year_returns.rank(ascending=False, method='min')\n",
    "    ranking_df.loc[year] = ranks\n",
    "\n",
    "print(\"\\nRanking by Annual Returns (1=Best, 4=Worst):\")\n",
    "print(ranking_df.astype(int))\n",
    "\n",
    "# Average ranking\n",
    "avg_ranking = ranking_df.mean().sort_values()\n",
    "print(f\"\\nAverage Ranking Across All Years:\")\n",
    "print(\"-\" * 40)\n",
    "for portfolio, avg_rank in avg_ranking.items():\n",
    "    print(f\"{portfolio}: {avg_rank:.2f}\")\n",
    "\n",
    "# Win rate analysis\n",
    "print(f\"\\nAnnual Win Rate (% of years ranked #1):\")\n",
    "print(\"-\" * 45)\n",
    "for portfolio in portfolios.keys():\n",
    "    win_rate = (ranking_df[portfolio] == 1).sum() / len(years) * 100\n",
    "    print(f\"{portfolio}: {win_rate:.1f}%\")\n",
    "\n",
    "# Multi-year consistency analysis\n",
    "print(f\"\\nConsistency Analysis:\")\n",
    "print(\"-\" * 25)\n",
    "for portfolio in portfolios.keys():\n",
    "    returns_series = yearly_comparison['Total Return (%)'][portfolio].dropna()\n",
    "    if len(returns_series) > 1:\n",
    "        consistency = returns_series.std()\n",
    "        print(f\"{portfolio} - Return Std Dev: {consistency:.2f}%\")\n",
    "\n",
    "# Best and worst years\n",
    "print(f\"\\nBest and Worst Years:\")\n",
    "print(\"-\" * 25)\n",
    "for portfolio in portfolios.keys():\n",
    "    returns_series = yearly_comparison['Total Return (%)'][portfolio].dropna()\n",
    "    if len(returns_series) > 0:\n",
    "        best_year = returns_series.idxmax()\n",
    "        worst_year = returns_series.idxmin()\n",
    "        print(f\"{portfolio}:\")\n",
    "        print(f\"  Best: {best_year} ({returns_series[best_year]:.2f}%)\")\n",
    "        print(f\"  Worst: {worst_year} ({returns_series[worst_year]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- เริ่มโค้ดสำหรับบันทึก Excel ---\n",
    "output_excel_path = f\"{output_dir}/{train_type}/03_Yearly_{model_type}_{pe_type}_{pre_post}_run_{run_no}.xlsx\"\n",
    "\n",
    "for metric in all_metrics:\n",
    "    sheet_name = metric.replace(\" \", \"_\")\n",
    "    df_to_save = yearly_comparison[metric].round(4)\n",
    "    save_dataframe_to_new_sheet(df_to_save, output_excel_path, sheet_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04230e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_comparison['Sortino Ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d858fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('say \"All code has finished\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
